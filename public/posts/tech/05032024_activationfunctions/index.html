<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Transformer Activation Functions and their Details | JoeLogs</title>
<meta name=keywords content="llm,math"><meta name=description content="Revisiting Activation Functions in Pytorch and their details"><meta name=author content="Sathvik Joel"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.f9589a61e1bfb8dc772aa8afbcf34ad46fa01c2148078d70b4d0907374e82745.css integrity="sha256-+ViaYeG/uNx3KqivvPNK1G+gHCFIB41wtNCQc3ToJ0U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/tech/05032024_activationfunctions/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]},chtml:{scale:1.2},svg:{scale:1.2}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Josefin+Sans:ital,wght@0,100..700;1,100..700&display=swap" rel=stylesheet><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&family=Josefin+Sans:ital,wght@0,100..700;1,100..700&display=swap" rel=stylesheet><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Transformer Activation Functions and their Details"><meta property="og:description" content="Revisiting Activation Functions in Pytorch and their details"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/tech/05032024_activationfunctions/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-05T11:30:03+00:00"><meta property="article:modified_time" content="2024-03-05T11:30:03+00:00"><meta property="og:site_name" content="JoeLogs"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Transformer Activation Functions and their Details"><meta name=twitter:description content="Revisiting Activation Functions in Pytorch and their details"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Postsüìö","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Techüë®‚Äçüíª","item":"http://localhost:1313/posts/tech/"},{"@type":"ListItem","position":3,"name":"Transformer Activation Functions and their Details","item":"http://localhost:1313/posts/tech/05032024_activationfunctions/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformer Activation Functions and their Details","name":"Transformer Activation Functions and their Details","description":"Revisiting Activation Functions in Pytorch and their details","keywords":["llm","math"],"articleBody":"Here are a few observations:\nGPT-2, developed by OpenAI, opts for the GELU (Gaussian Error Linear Unit) activation function\nOn the other hand, LLaMA, a creation of Facebook Research, embraces SwiGLU activation function.\nMeanwhile, Gemma, a PyTorch implementation by Google, adopts GeGLU activation functions.\nSo what are these new activation functions ? How should one go about implementing them in pytorch ?\nIn this blog post I try to understand the definitions of these activation functions and how they could be implemented in pytorch. At the end I will show the definitions from the actual implementaions in GPT-2, Gemma and LLaMA.\nImports import torch import torch.nn as nn Create a new tensor with random values to use it as input for the activation functions\ntensor = torch.randn(10) print(tensor) print(tensor.shape) tensor([-0.8281, 1.0340, -0.4363, -0.4764, 0.6419, -0.1156, 1.4339, 1.5654, 0.7124, -0.5667]) torch.Size([10]) RELU Variants RELU Rectified Linear Unit\n$$ f(x) = max(0, x) $$\nrelu = nn.ReLU() output = relu(tensor) print(output) tensor([0.0000, 1.0340, 0.0000, 0.0000, 0.6419, 0.0000, 1.4339, 1.5654, 0.7124, 0.0000]) CReLU Concatenated ReLU\n$$ f(x) = [\\text{ReLU}(x), \\text{ReLU}(-x)] $$\nObserve that the dimension of the output tensor is twice the input tensor.\ncrelu_output = torch.cat((relu(tensor), relu(-tensor))) print(crelu_output) tensor([0.0000, 1.0340, 0.0000, 0.0000, 0.6419, 0.0000, 1.4339, 1.5654, 0.7124, 0.0000, 0.8281, 0.0000, 0.4363, 0.4764, 0.0000, 0.1156, 0.0000, 0.0000, 0.0000, 0.5667]) Leaky ReLU $$ f(x) = \\begin{cases} x, \u0026 \\text{if } x \u003e 0\\\\ \\text{negative_slope } * x, \u0026 \\text{otherwise} \\end{cases} $$\nleaky_relu = nn.LeakyReLU( negative_slope=0.1) output = leaky_relu(tensor) print(output) tensor([-0.0828, 1.0340, -0.0436, -0.0476, 0.6419, -0.0116, 1.4339, 1.5654, 0.7124, -0.0567]) ReLU6 $$ f(x) = \\begin{cases} 0, \u0026 \\text{if } x \\leq 0\\\\ 6, \u0026 \\text{if } x \\geq 6\\\\ x, \u0026 \\text{otherwise} \\end{cases} $$\nrelu6 = nn.ReLU6() output = relu6(tensor) print(output) tensor([0.0000, 0.8944, 0.0000, 0.6875, 0.0526, 0.0000, 0.0000, 0.0000, 1.1088, 0.0000]) (Left) Leaky ReLU, (Middle) ReLU, (Right)ReLU6; taken from pytorch documentation\nOther Linear Unit Variants GELU Gaussian Error Linear Unit\n$$ GELU(x) = x \\times \\Phi(x) $$\nwhere $\\Phi(x)$ is the cumulative distribution function of the standard normal distribution (mean = 0, standard deviation = 1)\nThere is an approximate version of GELU that is faster to compute but at the cost of exactness\n$$ GELU(x) = 0.5 \\times x \\times (1 + \\tanh(\\sqrt{2/\\pi} \\times (x + 0.044715 \\times x^3))) $$\nor\n$$ GELU(x) = x \\times \\sigma(1.702 \\times x) $$\nMotivation for GELU üí° The motivation mentioned in the paper is based on the following observations:\nReLU determinstaically multiplies by 0 or 1 Dropout ( A regularization Technique ) also multiplies by 0 or 1, but stochastically It is possible to multiple with the 0-1 mask stochastically while also depending on the input in the following way ( this is similar to Adaptive Dropout, zoneout ) the mask is given by $m \\sim Bernoulli(\\Phi(x))$ Since $x\\sim N(0, 1)$ after the batch normaliztion anyway, this means that inputs have high probablity of getting dropped when $x$ decreases.\nThey say that we often want deterministic decision from the output, so they proposed GELU as the expected transformation of the stochastic regularizer.\n$$ E[x*m] = Ix * \\Phi(x) + 0x * (1 - \\Phi(x)) = x * \\Phi(x) $$\nI dont fully understand the motivation myself, but I guess having a partial idea is better than having no idea. They somehow want to take idea from dropout and activation functions and combine them to get a better activation function.\ngelu = nn.GELU(approximate=False) # using the accurate version of GELU output = gelu(tensor) print(output) tensor([-0.1688, 0.8783, -0.1445, -0.1510, 0.4747, -0.0525, 1.3252, 1.4735, 0.5428, -0.1618]) SiLU / Swish Sigmoid Linear Unit\n$$ f(x) = x \\times \\sigma(x) $$\nwhere $\\sigma(x)$ is the sigmoid function.\nThis is very similar to GELU but $\\sigma(x)$ is used instead of $\\Phi(x)$\nThere is also a version of Swish with learneaable parameter.\n$$ f(x) = x \\times \\sigma(\\beta x) $$\nwhere $\\beta$ is a learnable parameter\nsilu = nn.SiLU() output = silu(tensor) print(output) tensor([-0.2518, 0.7628, -0.1713, -0.1825, 0.4206, -0.0544, 1.1579, 1.2948, 0.4780, -0.2051]) HardSwish Introduced in Searching for MobileNetV3\n$$ f(x) = \\frac{x \\times \\text{ReLU6}(x + 3)}{6} $$\nIt is actually implemented piecewise as follows:\n$$ f(x) = \\begin{cases} 0, \u0026 \\text{if } x \\leq -3\\\\ x, \u0026 \\text{if } x \\geq +3\\\\ \\frac{x.(x+3)}{6}, \u0026 \\text{otherwise} \\end{cases} $$\n(Left)GELU, SiLU graphs from here ; (Right)Hard Swish from pytorch Documentation\nGLU and variants This section is heavily based on the paper GLU Variants improve Transfomers\nGLU Gated Linear Unit (GLU) : A neural network layer defined by component-wise product of two linear transformations of the input\n$$ \\text{GLU}(x, W, V, b, c) = \\sigma (Wx + b) \\odot (Vx + c) $$\nThey also suggest omitting the activation, whih they call a bilnear layer\n$$ \\text{Bilinear}(x, W, V, b, c) = (Wx + b) \\odot (Vx + c) $$\nNote: The bias term is often omitted\nGLU Variants Any activation function could be used in place of $\\sigma$ in the GLU equation, giving rise to a family of GLU variants.\n$$ \\text{ReGLU}(x, W, V, b, c) = \\text{ReLU}(Wx + b) \\odot (Vx + c) $$ $$ \\text{SiGLU}(x, W, V, b, c) = \\text{SiLU(Wx + b)} \\odot (Vx + c)\\\\ $$ $$ \\text{GeGLU}(x, W, V, b, c) = \\text{GELU}(Wx + b) \\odot (Vx + c)\\\\ $$\nExample Practical Implementation # ReGLU could be implmeneted like this in pytorch class ReGLU(nn.Module): def __init__(self): super(ReGLU, self).__init__() input_dim = 10 hidden_dim = 20 self.W = nn.Linear(input_dim, hidden_dim) self.V = nn.Linear(input_dim, hidden_dim) self.relu = nn.ReLU() def forward(self, x): return self.W(x) * self.relu(self.V(x)) # Create a tensor with 10 random numbers tensor = torch.randn(10) # Create an instance of the ReGLU class reglu = ReGLU() # Apply the ReGLU function to the tensor output = reglu(tensor) print(output) tensor([ 0.0000, 0.0074, 0.0000, -0.0000, -0.0753, -0.0598, -0.0000, -0.0000, -0.0000, 0.1110, -0.0109, 0.2933, -0.0185, 0.0000, -0.0016, 0.0250, 0.0000, 0.3512, 0.0000, 0.0000], grad_fn=) Implementations from Projects Here are a few practical implementations from LLM models\nGPT-2 GELU implementaiton from GPT-2 model definition.\nGPT-2 uses an approximate version of GELU.\nLLaMA SwiGLU implementaiton from LLaMA model definition.\nThe python code\nF.silu(self.w1(x)) * self.w3(x) is the SwiGLU implementation, the whole function is for the FFN ( MLP layer ) in the transformers.\nGemma GeGLU implementaiton from Gemma model definition.\nThe GEGLU is a little more hidden in this function, the screenshot shows the whole FFN ( MLP layer ) function, but if you carefully observe you can make out the GEGLU implementation. (Hint look at the fuse variable)\n","wordCount":"1071","inLanguage":"en","datePublished":"2024-03-05T11:30:03Z","dateModified":"2024-03-05T11:30:03Z","author":{"@type":"Person","name":"Sathvik Joel"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/tech/05032024_activationfunctions/"},"publisher":{"@type":"Organization","name":"JoeLogs","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Joe'Logs (Alt + H)">Joe'Logs</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/search/ title="Searchüîç (Alt + /)" accesskey=/><span>Searchüîç</span></a></li><li><a href=http://localhost:1313/posts/ title=Postsüìö><span>Postsüìö</span></a></li><li><a href=http://localhost:1313/archives/ title=Archives‚è±Ô∏è><span>Archives‚è±Ô∏è</span></a></li><li><a href=http://localhost:1313/tags/ title=Tagsüè∑Ô∏è><span>Tagsüè∑Ô∏è</span></a></li><li><a href=http://localhost:1313/posts/life title=Lifeüå±><span>Lifeüå±</span></a></li><li><a href=http://localhost:1313/posts/tech title=Techüë®‚Äçüíª><span>Techüë®‚Äçüíª</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/>Postsüìö</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/tech/>Techüë®‚Äçüíª</a></div><h1 class=post-title>Transformer Activation Functions and their Details</h1><div class=post-description>Revisiting Activation Functions in Pytorch and their details</div><div class=post-meta>&lt;span title='2024-03-05 11:30:03 +0000 UTC'>March 5, 2024&lt;/span>&amp;nbsp;¬∑&amp;nbsp;6 min&amp;nbsp;¬∑&amp;nbsp;Sathvik Joel</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#imports aria-label=Imports>Imports</a></li><li><a href=#relu-variants aria-label="RELU Variants">RELU Variants</a><ul><li><a href=#relu aria-label=RELU>RELU</a></li><li><a href=#crelu aria-label=CReLU>CReLU</a></li><li><a href=#leaky-relu aria-label="Leaky ReLU">Leaky ReLU</a></li><li><a href=#relu6 aria-label=ReLU6>ReLU6</a></li></ul></li><li><a href=#other-linear-unit-variants aria-label="Other Linear Unit Variants">Other Linear Unit Variants</a><ul><li><a href=#gelu aria-label=GELU>GELU</a></li><li><a href=#silu--swish aria-label="SiLU / Swish">SiLU / Swish</a></li><li><a href=#hardswish aria-label=HardSwish>HardSwish</a></li></ul></li><li><a href=#glu-and-variants aria-label="GLU and variants">GLU and variants</a><ul><li><a href=#glu aria-label=GLU>GLU</a></li><li><a href=#glu-variants aria-label="GLU Variants">GLU Variants</a></li><li><a href=#example-practical-implementation aria-label="Example Practical Implementation">Example Practical Implementation</a></li></ul></li><li><a href=#implementations-from-projects aria-label="Implementations from Projects">Implementations from Projects</a><ul><li><a href=#gpt-2 aria-label=GPT-2>GPT-2</a></li><li><a href=#llama aria-label=LLaMA>LLaMA</a></li><li><a href=#gemma aria-label=Gemma>Gemma</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>Here are a few observations:</p><p>GPT-2, developed by OpenAI, opts for the GELU (Gaussian Error Linear Unit) activation function</p><p>On the other hand, LLaMA, a creation of Facebook Research, embraces SwiGLU activation function.</p><p>Meanwhile, Gemma, a PyTorch implementation by Google, adopts GeGLU activation functions.</p><p>So what are these new activation functions ?
How should one go about implementing them in pytorch ?</p><p>In this blog post I try to understand the definitions of these activation functions and how they could be implemented in pytorch. At the end I will show the definitions from the actual implementaions in GPT-2, Gemma and LLaMA.</p><h2 id=imports>Imports<a hidden class=anchor aria-hidden=true href=#imports>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span></code></pre></div><p>Create a new tensor with random values to use it as input for the activation functions</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tensor</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>tensor</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor([-0.8281,  1.0340, -0.4363, -0.4764,  0.6419, -0.1156,  1.4339,  1.5654,
         0.7124, -0.5667])
torch.Size([10])
</code></pre><h2 id=relu-variants>RELU Variants<a hidden class=anchor aria-hidden=true href=#relu-variants>#</a></h2><h3 id=relu>RELU<a hidden class=anchor aria-hidden=true href=#relu>#</a></h3><p>Rectified Linear Unit</p><p>$$
f(x) = max(0, x)
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>relu</span><span class=p>(</span><span class=n>tensor</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor([0.0000, 1.0340, 0.0000, 0.0000, 0.6419, 0.0000, 1.4339, 1.5654, 0.7124,
        0.0000])
</code></pre><h3 id=crelu>CReLU<a hidden class=anchor aria-hidden=true href=#crelu>#</a></h3><p>Concatenated ReLU</p><p>$$
f(x) = [\text{ReLU}(x), \text{ReLU}(-x)]
$$</p><p>Observe that the dimension of the output tensor is twice the input tensor.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>crelu_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>relu</span><span class=p>(</span><span class=n>tensor</span><span class=p>),</span> <span class=n>relu</span><span class=p>(</span><span class=o>-</span><span class=n>tensor</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>crelu_output</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor([0.0000, 1.0340, 0.0000, 0.0000, 0.6419, 0.0000, 1.4339, 1.5654, 0.7124,
        0.0000, 0.8281, 0.0000, 0.4363, 0.4764, 0.0000, 0.1156, 0.0000, 0.0000,
        0.0000, 0.5667])
</code></pre><h3 id=leaky-relu>Leaky ReLU<a hidden class=anchor aria-hidden=true href=#leaky-relu>#</a></h3><p>$$
f(x) = \begin{cases}
x, & \text{if } x > 0\\
\text{negative_slope } * x, & \text{otherwise}
\end{cases}
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>leaky_relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LeakyReLU</span><span class=p>(</span> <span class=n>negative_slope</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>leaky_relu</span><span class=p>(</span><span class=n>tensor</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor([-0.0828,  1.0340, -0.0436, -0.0476,  0.6419, -0.0116,  1.4339,  1.5654,
         0.7124, -0.0567])
</code></pre><h3 id=relu6>ReLU6<a hidden class=anchor aria-hidden=true href=#relu6>#</a></h3><p>$$
f(x) = \begin{cases}
0, & \text{if } x \leq 0\\
6, & \text{if } x \geq 6\\
x, & \text{otherwise}
\end{cases}
$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>relu6</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU6</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>relu6</span><span class=p>(</span><span class=n>tensor</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor([0.0000, 0.8944, 0.0000, 0.6875, 0.0526, 0.0000, 0.0000, 0.0000, 1.1088,
    0.0000])
</code></pre><figure><img loading=lazy src=images/relu_variants2.jpg alt="(Left) Leaky ReLU, (Middle) ReLU, (Right)ReLU6; taken from pytorch documentation"><figcaption><p>(Left) Leaky ReLU, (Middle) ReLU, (Right)ReLU6; taken from pytorch documentation</p></figcaption></figure><h2 id=other-linear-unit-variants>Other Linear Unit Variants<a hidden class=anchor aria-hidden=true href=#other-linear-unit-variants>#</a></h2><h3 id=gelu>GELU<a hidden class=anchor aria-hidden=true href=#gelu>#</a></h3><p>Gaussian Error Linear Unit</p><p>$$
GELU(x) = x \times \Phi(x)
$$</p><p>where $\Phi(x)$ is the cumulative distribution function of the standard normal distribution (mean = 0, standard deviation = 1)</p><p>There is an approximate version of GELU that is faster to compute but at the cost of exactness</p><p>$$
GELU(x) = 0.5 \times x \times (1 + \tanh(\sqrt{2/\pi} \times (x + 0.044715 \times x^3)))
$$</p><p>or</p><p>$$
GELU(x) = x \times \sigma(1.702 \times x)
$$</p><div id=motivation_gelu class=toc style=margin-bottom:20px;background:var(--theme)><details><summary><div class=details>Motivation for GELU üí°</div></summary><div class=inner><p>The motivation mentioned in the <a href=https://arxiv.org/abs/1606.08415>paper</a> is based on the following observations:</p><ol><li>ReLU determinstaically multiplies by 0 or 1</li><li>Dropout ( A regularization Technique ) also multiplies by 0 or 1, but stochastically</li><li>It is possible to multiple with the 0-1 mask stochastically while also depending on the input in the following way ( this is similar to Adaptive Dropout, zoneout ) the mask is given by $m \sim Bernoulli(\Phi(x))$</li></ol><p>Since $x\sim N(0, 1)$ after the batch normaliztion anyway, this means that inputs have high probablity of getting dropped when $x$ decreases.</p><p>They say that we often want deterministic decision from the output, so they proposed GELU as the expected transformation of the stochastic regularizer.</p><p>$$
E[x*m] = Ix * \Phi(x) + 0x * (1 - \Phi(x)) = x * \Phi(x)
$$</p><blockquote><p>I dont fully understand the motivation myself, but I guess having a partial idea is better than having no idea. They somehow want to take idea from dropout and activation functions and combine them to get a better activation function.</p></blockquote></div></details></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>gelu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>GELU</span><span class=p>(</span><span class=n>approximate</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span> <span class=c1># using the accurate version of GELU</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>gelu</span><span class=p>(</span><span class=n>tensor</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor([-0.1688,  0.8783, -0.1445, -0.1510,  0.4747, -0.0525,  1.3252,  1.4735,
         0.5428, -0.1618])
</code></pre><h3 id=silu--swish>SiLU / Swish<a hidden class=anchor aria-hidden=true href=#silu--swish>#</a></h3><p>Sigmoid Linear Unit</p><p>$$
f(x) = x \times \sigma(x)
$$</p><p>where $\sigma(x)$ is the sigmoid function.</p><p>This is very similar to GELU but $\sigma(x)$ is used instead of $\Phi(x)$</p><p>There is also a version of Swish with learneaable parameter.</p><p>$$
f(x) = x \times \sigma(\beta x)
$$</p><p>where $\beta$ is a learnable parameter</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>silu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>SiLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>silu</span><span class=p>(</span><span class=n>tensor</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor([-0.2518,  0.7628, -0.1713, -0.1825,  0.4206, -0.0544,  1.1579,  1.2948,
         0.4780, -0.2051])
</code></pre><h3 id=hardswish>HardSwish<a hidden class=anchor aria-hidden=true href=#hardswish>#</a></h3><p>Introduced in <a href=https://arxiv.org/abs/1905.02244>Searching for MobileNetV3</a></p><p>$$
f(x) = \frac{x \times \text{ReLU6}(x + 3)}{6}
$$</p><p>It is actually implemented piecewise as follows:</p><p>$$
f(x) = \begin{cases}
0, & \text{if } x \leq -3\\
x, & \text{if } x \geq +3\\
\frac{x.(x+3)}{6}, & \text{otherwise}
\end{cases}
$$</p><figure><img loading=lazy src=images/linear_variants.jpg alt="(Left)GELU, SiLU graphs from here ; (Right)Hard Swish from pytorch Documentation"><figcaption><p>(Left)GELU, SiLU graphs from <a href=https://kikaben.com/swiglu-2020/>here</a> ; (Right)Hard Swish from pytorch Documentation</p></figcaption></figure><h2 id=glu-and-variants>GLU and variants<a hidden class=anchor aria-hidden=true href=#glu-and-variants>#</a></h2><p>This section is <strong>heavily</strong> based on the paper <a href=https://arxiv.org/pdf/2002.05202.pdf>GLU Variants improve Transfomers</a></p><h3 id=glu>GLU<a hidden class=anchor aria-hidden=true href=#glu>#</a></h3><p><strong>Gated Linear Unit (GLU)</strong> : A neural network layer defined by component-wise product of two linear transformations of the input</p><p>$$
\text{GLU}(x, W, V, b, c) = \sigma (Wx + b) \odot (Vx + c)
$$</p><p>They also suggest omitting the activation, whih they call a bilnear layer</p><p>$$
\text{Bilinear}(x, W, V, b, c) = (Wx + b) \odot (Vx + c)
$$</p><p>Note: The <code>bias</code> term is often omitted</p><h3 id=glu-variants>GLU Variants<a hidden class=anchor aria-hidden=true href=#glu-variants>#</a></h3><p>Any activation function could be used in place of $\sigma$ in the GLU equation, giving rise to a family of GLU variants.</p><p>$$
\text{ReGLU}(x, W, V, b, c) = \text{ReLU}(Wx + b) \odot (Vx + c)
$$
$$
\text{SiGLU}(x, W, V, b, c) = \text{SiLU(Wx + b)} \odot (Vx + c)\\
$$
$$
\text{GeGLU}(x, W, V, b, c) = \text{GELU}(Wx + b) \odot (Vx + c)\\
$$</p><h3 id=example-practical-implementation>Example Practical Implementation<a hidden class=anchor aria-hidden=true href=#example-practical-implementation>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># ReGLU could be implmeneted like this in pytorch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>ReGLU</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>ReGLU</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>input_dim</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_dim</span> <span class=o>=</span> <span class=mi>20</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>V</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>hidden_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>relu</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>W</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>V</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create a tensor with 10 random numbers</span>
</span></span><span class=line><span class=cl><span class=n>tensor</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Create an instance of the ReGLU class</span>
</span></span><span class=line><span class=cl><span class=n>reglu</span> <span class=o>=</span> <span class=n>ReGLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Apply the ReGLU function to the tensor</span>
</span></span><span class=line><span class=cl><span class=n>output</span> <span class=o>=</span> <span class=n>reglu</span><span class=p>(</span><span class=n>tensor</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor([ 0.0000,  0.0074,  0.0000, -0.0000, -0.0753, -0.0598, -0.0000, -0.0000,
        -0.0000,  0.1110, -0.0109,  0.2933, -0.0185,  0.0000, -0.0016,  0.0250,
         0.0000,  0.3512,  0.0000,  0.0000], grad_fn=&lt;MulBackward0&gt;)
</code></pre><h2 id=implementations-from-projects>Implementations from Projects<a hidden class=anchor aria-hidden=true href=#implementations-from-projects>#</a></h2><p>Here are a few practical implementations from LLM models</p><h3 id=gpt-2>GPT-2<a hidden class=anchor aria-hidden=true href=#gpt-2>#</a></h3><figure><img loading=lazy src=images/gelu.jpg alt="GELU implementaiton from GPT-2 model definition."><figcaption><p>GELU implementaiton from <a href=https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/model.py#L25>GPT-2 model definition</a>.</p></figcaption></figure><p>GPT-2 uses an approximate version of GELU.</p><h3 id=llama>LLaMA<a hidden class=anchor aria-hidden=true href=#llama>#</a></h3><figure><img loading=lazy src=images/llama.jpg alt="SwiGLU implementaiton from LLaMA model definition."><figcaption><p>SwiGLU implementaiton from <a href=https://github.com/facebookresearch/llama/blob/a0a4da8b497c566403941ceec47c2512ecf9dd20/llama/model.py#L348>LLaMA model definition</a>.</p></figcaption></figure><p>The python code</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>F</span><span class=o>.</span><span class=n>silu</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>w1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>w3</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span></code></pre></div><p>is the SwiGLU implementation, the whole function is for the FFN ( MLP layer ) in the transformers.</p><h3 id=gemma>Gemma<a hidden class=anchor aria-hidden=true href=#gemma>#</a></h3><figure><img loading=lazy src=images/Gemma.jpg alt="GeGLU implementaiton from Gemma model definition."><figcaption><p>GeGLU implementaiton from <a href=https://github.com/google/gemma_pytorch/blob/324cb185aa3fe60f43dad4b7ce5096ffdd513cb1/gemma/model.py#L195>Gemma model definition</a>.</p></figcaption></figure><p>The GEGLU is a little more hidden in this function, the screenshot shows the whole FFN ( MLP layer ) function, but if you carefully observe you can make out the GEGLU implementation. (Hint look at the <code>fuse</code> variable)</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/llm/>Llm</a></li><li><a href=http://localhost:1313/tags/math/>Math</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/tech/12032024_forest/><span class=title>¬´ Prev</span><br><span>Forest in Latex</span>
</a><a class=next href=http://localhost:1313/posts/tech/04032024_tokenizer/><span class=title>Next ¬ª</span><br><span>Understanding Andrejs Tokenizer Video</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>JoeLogs</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>