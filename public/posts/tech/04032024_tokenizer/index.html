<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understanding Andrejs Tokenizer Video | JoeLogs</title>
<meta name=keywords content="llm,math"><meta name=description content="Decoding details from Andrejs video on Tokenizer"><meta name=author content="Sathvik Joel"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.f9589a61e1bfb8dc772aa8afbcf34ad46fa01c2148078d70b4d0907374e82745.css integrity="sha256-+ViaYeG/uNx3KqivvPNK1G+gHCFIB41wtNCQc3ToJ0U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/tech/04032024_tokenizer/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]},chtml:{scale:1.2},svg:{scale:1.2}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Josefin+Sans:ital,wght@0,100..700;1,100..700&display=swap" rel=stylesheet><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@300..700&family=Josefin+Sans:ital,wght@0,100..700;1,100..700&display=swap" rel=stylesheet><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Understanding Andrejs Tokenizer Video"><meta property="og:description" content="Decoding details from Andrejs video on Tokenizer"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/tech/04032024_tokenizer/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-03-04T11:30:03+00:00"><meta property="article:modified_time" content="2024-03-04T11:30:03+00:00"><meta property="og:site_name" content="JoeLogs"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Understanding Andrejs Tokenizer Video"><meta name=twitter:description content="Decoding details from Andrejs video on Tokenizer"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Postsüìö","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Techüë®‚Äçüíª","item":"http://localhost:1313/posts/tech/"},{"@type":"ListItem","position":3,"name":"Understanding Andrejs Tokenizer Video","item":"http://localhost:1313/posts/tech/04032024_tokenizer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding Andrejs Tokenizer Video","name":"Understanding Andrejs Tokenizer Video","description":"Decoding details from Andrejs video on Tokenizer","keywords":["llm","math"],"articleBody":"This blog is based on Andrejs video on TokenizerüìΩÔ∏è. In an interview with Lex Fridman, Andrej said, he takes 10 hrs to make 1 hr of content, this is me trying to decode the rest of the 9 hours.\nIn this I wont be concentrating on the python implementation of BPE algorithm itself. The idea is to uncover the details and subtle points around the video and look closely in this very dense video lecture by Andrej.\nUNICODE AND UTF-8 The whole BPE algorithm is based on the UNICODE and UTF-8 encoding, in this I present a few details that are relavent to understand the BPE algorithm.\nIn the video Andrej refer to the UNICODE and UTF-8 wikipedia pages, along with this blog post that is more redable. But I persoanlly found this amazing blog that is more reader friendly and gives an easy understanding.\nImportant Points about UNICODE and UTF-8 Here are the brief points that are really relavent, taken from this amazing blog\nUNICODE is a table that assigns unique numbers to different characters, they keep updating and adding new characters to the table. UTF-8 is a way to encode those numbers into bytes. Simply, the way to store the UNICODE characters in memory. UTF-8 is a variable length encoding, meaning it can use 1, 2, 3 or 4 bytes to store a character. UTF-8 is backward compatible with ASCII, meaning that any ASCII text is also a valid UTF-8 text. Again please refer to the resourced above to understand advanced concepts like Grapheme clusters, I persoanlly found the details really worth knowing.\nRelated Python Functions ord(\"Ìïú\") # Gives the UNICODE number of the character # 54620 chr(54620) # Gives the character corresponding to the UNICODE number # 'Ìïú' # UTF-8 encoding \"ÏïàÎÖïÌïòÏÑ∏Ïöî is hello in korean\".encode(\"utf-8\") # b'\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94 is hello in korean' # This gives the bytes representation of the string, use list on the output to get the list in the form of integers list(\"ÏïàÎÖïÌïòÏÑ∏Ïöî is hello in korean\".encode(\"utf-8\")) # [236, 149, 136, ... 101, 97, 110] # UTF-8 decoding b\"\\xec\\x95\\x88\\xeb\\x85\\x95\\xed\\x95\\x98\\xec\\x84\\xb8\\xec\\x9a\\x94 is hello in korean\".decode('utf-8') # 'ÏïàÎÖïÌïòÏÑ∏Ïöî is hello in korean' # If you have a list of integers then use the below method bytes([236, 149, 136, 235, 133, 149, 237, 149, 156, 236, 151, 144, 32, 105, 115, 32, 104, 101, 108, 108, 111, 32, 105, 110, 32, 107, 111, 114, 101, 97, 110]).decode(\"utf-8\") # 'ÏïàÎÖïÌïòÏÑ∏Ïöî is hello in korean' Tokenizer This amazing website lets you visualize the Tokenization, this is showcased by andrej in the video. You can play with differnt tokenizers and see how they tokenize the given text.\nLeaky ReLU activation function, picture taken from pytorch documentaion of LeakyReLU.\nObservationsüí° Each space is tokenized separately in the python indentaion, this make GPT-2 worse in coding tasks. There is no defined rules for tokenizing numbers, they tokenized very randonly The word EGG is tokenized differently based on the location in the sentence, casing oof the word Korean takes a lot more tokens than english, this is very important observaion, the reason for this is the less amount of korean in the traning data of BPE. BPE Introduction BPE is a tokenizer that tokenizes the text into subwords.\nWhat is Tokenization ?\nTokenization is the process of breaking the text into smaller parts called tokens. These tokens can be words, characters, or subwords.\nThere are various ways to tokenization a text :\nRule based tokenization, this uncludes using regex to split the text into tokens. Subword tokenization, this includes breaking the text into subwords, this is what BPE does. This wonderful blog dives deep into different tokenization techniques, and is a must read to understand where exactly BPE falls in the whole space of tokenization.\nMotivation Andrej cites that the first mention of BPE in NLP was here. They first introduced BPE in the context of NMT (Neural Machine transaltion), as at that time most of the NMT models operated on fixed vocabularies ( not the vocabulary in the context of LLMs, they were using word based vacabularies). But translation is an open vocalbulary problem, meaning that the model should be able to translate any word in any language ( not just the ones in their vocabulary). Their motivation comes from the idea that, humans could translate unkown words by breaking them down into smaller parts, and translating those parts, so a sub word segmentation is ideal. They chose BPE and apply it at a chracter level ( unlike at the byte level, that is used for LLMs ).\nAlogorithm This explanation is based on the wikipedia page\nThe simple version goes like this :\nIteratively replace the most frequent pair of consecutive symbols with a new symbol, until the desired number of merge operations is reached.\nBPE Example üí° Here is an example from the same wikipedia page\nSuppose the data to be encoded is\naaabdaaabac\nThe byte pair ‚Äúaa‚Äù occurs most often, so it will be replaced by a byte that is not used in the data, such as ‚ÄúZ‚Äù. Now there is the following data and replacement table:\nZabdZabac\nZ=aa\nThen the process is repeated with byte pair ‚Äúab‚Äù, replacing it with ‚ÄúY‚Äù:\nZYdZYac\nY=ab\nZ=aa\nThe only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing ‚ÄúZY‚Äù with ‚ÄúX‚Äù:\nXdXac\nX=ZY\nY=ab\nZ=aa\nThis data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.\nThis algorithm is general and can be applied at any level, depending on your definition of a symbol. In the context of LLMs we use bytes as the symbols. So we merge bytes.\nAndrej also spends 30-40 mins providing a python implementation of the BPE from scratch, I tried it and it was a fun exerciese.\nHere is an imporant observation when BPE is applied to text, the number of merges is a hyper-parameter\n$\\uparrow$ the number of merges, $\\uparrow$ the vocabulary size, meaning the size of the embedding layer and the softmax layer increases. $\\downarrow$ the number of merges, $\\downarrow$ the vocabulary size, meaning that the same text would now would be tokenized into more number of tokens, but attention is costly and we want to attend to the same amount of information keeping the attention cost low. Subtle modifications to BPE So BPE is just endcoding the given string in utf-8 and then applying the BPE algorithm on the bytes, right?\nYou could do it, and thats exaclty what the Basetokensizer() does in minBPE implementation.\nBut GPT-2 paper mentions a problem with this, and here is the problem ( Taken from the paper directly )\n‚ÄúWe observed BPE including many versions of common words like dog since they occur in many variations such as dog. dog! dog? . This results in a sub-optimal allocation of limited vocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any byte sequence.‚Äù\nThe final vocabulary size is a hyper-parameter, and this is a trade-off beecause, T\nThe larger it is, the bigger the embedding layer is and also the softmax layer gets diluted when making the prediction at the head of the model. So, we really dont want to waste the vocabulary slots on the variations of esentially the same word.\nHence, they restrict the BPE algorithm to not merge across character categories.\nHow do they do this ?\nWhile traning, they first split the string using a regex. Below are the regexes taken from the minBPE.\nRegexs # the main GPT text split patterns, see # https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py GPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\" GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\" So before we train, we first chunk the training text using re.findall, then only count the pair frequencies in these chunks, this avoids the merges across the character categories. ( See the changes to train function in RegexTokenizer in minBPE ). Essentially sinece ‚Äúdog?‚Äù gets split into [\"dog\", \"?\"] we never would have merged ‚Äúdog?‚Äù into a single token. ( It took some time for me to understand this, but I hope I got it right )\nThe same is done while encoding as well, we chunk, then encode and then merge the tokens.\nNote : GPT-2 never relased their training code for BPE (only inference code is availale), the implementation with the regex wont make it complete because, you can observe that in GPT-2 tokenizer tokenizes all spaces independently, but applying the regex and training wont achieve that, this implies that they are enforcing more rules that are not clear.\nWhat constitutes a trained tokenizer All that is needed to encode and decode are just two things. The vocab and the merges.\nvocab is a dictionary that maps the index to the bytes string and merges is a dictionary that maps the pair of tokens to the new token id.\nHere is the merges and vocab from the GPT-2\n!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe !wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json import os, json with open('encoder.json', 'r') as f: encoder = json.load(f) # \u003c--- ~equivalent to our \"vocab\" with open('vocab.bpe', 'r', encoding=\"utf-8\") as f: bpe_data = f.read() bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]] # ^---- ~equivalent to our \"merges\" Using tiktoken tiktoken is the offical library relased by OpenAI for tokenization, it doesnt contain the training code itself but we can use it to encode and decode with their tokenizers.\nUsing Tiktoken import tiktoken # GPT-2 (does not merge spaces) enc = tiktoken.get_encoding(\"gpt2\") print(enc.encode(\" hello world!!!\")) # GPT-4 (merges spaces) enc = tiktoken.get_encoding(\"cl100k_base\") print(enc.encode(\" hello world!!!\")) Special Tokens len(encode) # 50257 If we had 50,000 merges and our base vocabulary was 256 bytes, then we would have 50,256 tokens. But we have 50,257 tokens, this is because of the special token \u003c|endoftext|\u003e\nWhen this token is present in the text it is tokenized as a single token and this is not part of BPE algorithm itself and need to be handled speparetly.\nGPT-2 only has one special token.\nSpeical Tokens for GPT-4 tokenizer taken from tiktoken library code\nGPT-4 has 4 special tokens, the FIM tokens are based on this paper\nNote : Tiktoken also supports addition of new tokens, refer to their documentation for more details.\nHow to add Special Tokens Some special tokens are added at the start of the training itself, but some are added after the pretrainig ( the research reasons are discussed later down the post ). Adding Special tokens involes a minor model surgery, these are the two steps involved\nAdd an extra row in teh embedding layer, and initialize it with random weights. Add an extra row in the softmax layer as well. This kind of addition is very common, once the model is pre-trained, these special tokens are especially important in later stages for fine-tuning when creating chat based models.\nSpeical Tokens for GPT-3.5-turbo. Observe how the \u003c|im_start|\u003e, \u003c|im_end|\u003e tokens are tokenized as a single token\nSome of the special tokens for llama tokenizer, see that , , are special tokens\n( TODO : Find an example in code for adding special tokens )\nThere is an entire design space around adding new tokens, here is a paper that uses this idea Learning to Compress Prompts with Gist Tokens as was mentioned by Andrej in the video.\nDifferent Modalities So the idea many people are converging to these days is not to change the architecture of the model, but to find a way to tokenize the modality into tokens and feed it to the model.\nFor example, in SORA by OpenAI they found a way to tokenize the videos into patches that could be feed to a Model.\nTaken from SORA Technical Paper\nminBPE The goal of minBPE library is to have tiktoken but with training code.\nSentencePiece SentencePiece is a library released by Google, it can is a text tokenizer and supports both BPE and Unigram Language Model( A different alogirhtm for tokenization ). Both LLaMA and Mistral uses SentencePiece for tokenization.\nSo the main differnce according to Andrej, between SentencePiece and tiktoken is that, the BPE in sentencePiece is performed directly on ‚ÄúUniCode CodePoints‚Äù and not at the level of Bytes.\nSo, the merges happen between codepoints.\nOptions character_coverage ( say 0.9995 ) parameter makes sure that if a unicode is rare then it is not included in the vocabulary.\nbyte_fallback parameter deals with the code points that are excluded from the vocabulary because of the character_coverage parameter. If a code point is not in the vocabulary, then it is encoded ( while infering ), as a sequence of bytes ( according to UTF-8 ), or marked as depending on the value of this parameter.\nExample Demonstrating the usage of Options ( Taken from the Video )\nOptions in Detail In the below excursion, we train and observe the practical implications of the options in the training of SentencePiece.\nTrainingüí° import sentencepiece as spm # write a toy.txt file with some random text with open(\"toy.txt\", \"w\", encoding=\"utf-8\") as f: f.write(\"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\") Observe that the training data doesn‚Äôt include any korean code points ( for example later )\n# train a sentencepiece model on it # the settings here are (best effort) those used for training Llama 2 import os options = dict( # input spec input=\"toy.txt\", input_format=\"text\", # output spec model_prefix=\"tok400\", # output filename prefix # algorithm spec # BPE alg model_type=\"bpe\", vocab_size=400, # normalization normalization_rule_name=\"identity\", # ew, turn off normalization remove_extra_whitespaces=False, input_sentence_size=200000000, # max number of training sentences max_sentence_length=4192, # max number of bytes per sentence seed_sentencepiece_size=1000000, shuffle_input_sentence=True, # rare word treatment character_coverage=0.99995, byte_fallback=True, # merge rules split_digits=True, split_by_unicode_script=True, split_by_whitespace=True, split_by_number=True, max_sentencepiece_length=16, add_dummy_prefix=True, allow_whitespace_only_pieces=True, # special tokens unk_id=0, # the UNK token MUST exist bos_id=1, # the others are optional, set to -1 to turn off eos_id=2, pad_id=-1, # systems num_threads=os.cpu_count(), # use ~all system resources ) spm.SentencePieceTrainer.train(**options) Configure the Trainer, byte_fallback is set to True\nIn the above drop down we have seen the code for training a SentencePiece model, now lets see how the vocab is organized and how the encoding works.\nsp = spm.SentencePieceProcessor() sp.load('tok400.model') vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())] vocab We have loaded the model as seen in the above code, we will now see how the vocalbulary is organized.\nGroup 1 : Special Tokens ['', 0], ['', 1], ['', 2], Group 2 : All the individual bytes because the byte_fallback is set to be true ['\u003c0x00\u003e', 3], ['\u003c0x01\u003e', 4], ['\u003c0x02\u003e', 5], ['\u003c0x03\u003e', 6], ['\u003c0x04\u003e', 7], ... ['\u003c0xFB\u003e', 254], ['\u003c0xFC\u003e', 255], ['\u003c0xFD\u003e', 256], ['\u003c0xFE\u003e', 257], ['\u003c0xFF\u003e', 258] Group 3 : The parent Tokens/Merged Tokens ['en', 259], ['‚ñÅt', 260], ['ce', 261], ['in', 262], ['ra', 263], ['‚ñÅm', 271], ['‚ñÅu', 272], ['entence', 276], ['‚ñÅthe', 294], ['Piece', 295], ['‚ñÅSentence', 296], ['‚ñÅSentencePiece', 297], ['.]', 298], ['Ne', 299], ['.])', 314], ['age', 315], ['del', 316], ['‚ñÅNe', 323], ['guage', 335], ['‚ñÅtraining', 343], ['.,', 344], ['BP', 345], ['Ku', 346], ['ab', 347], ['lo', 358], ['nr', 359], ['oc', 360] Group 4 : The Individual Code Points that are a part of the training text, the rare ones in the training text are exempted ['e', 361], ['‚ñÅ', 362], [',', 395], ['/', 396], ['B', 397], ['E', 398], ['K', 399] Now, lets encode ( infer )\nids = sp.encode(\"hello ÏïàÎÖïÌïòÏÑ∏Ïöî\") print(ids) [362, 378, 361, 372, 358, 362, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151] print([sp.id_to_piece(idx) for idx in ids]) ['‚ñÅ', 'h', 'e', 'l', 'lo', '‚ñÅ', '\u003c0xEC\u003e', '\u003c0x95\u003e', '\u003c0x88\u003e', '\u003c0xEB\u003e', '\u003c0x85\u003e', '\u003c0x95\u003e', '\u003c0xED\u003e', '\u003c0x95\u003e', '\u003c0x98\u003e', '\u003c0xEC\u003e', '\u003c0x84\u003e', '\u003c0xB8\u003e', '\u003c0xEC\u003e', '\u003c0x9A\u003e', '\u003c0x94\u003e'] See that the korean characters gets encoded as a sequence of bytes because, those code points done have any mapping in the vocabulary.\nHad the byte_fallback been set to False.\nids = sp.encode(\"hello ÏïàÎÖïÌïòÏÑ∏Ïöî\") print(ids) [362, 378, 361, 372, 358, 362, 0] Note that id 0 stands for UNK\nNotes on vocab size In BPE the vocab size is a hyper-parameter. Here are a few questions and answers to them as mentioned by Andrej in his video\nQ. Why cant the vocab size be infinite ?\nIn the model definition there are two places where the vocab size appers as show in the figure below:\nvocab size in model definition in gpt.py from andrejs minGPT repo\nSo as the vocab size increases\nThe embedding layer and the lm_head is going to grow in size, this means there are a lot of parameters This could mean that these parameters might be under trained, as the model is going to see these tokens more rarely. [ Imagine a rare word such as pneumonoultramicroscopicsilicovolcanoconiosis in the whole training data, the model is going to see this point only once, so the corresponding embedding is going to be under trained ] We are squising the information a lot, this is beneficial because we could attend to more information for the same amount of computation, but this also means that the model has to understand a lot of information in the forward pass, which could be a bottleneck. Tokenization Wieirdness Tokenization is at the heart of much weirdness of LLMs\nWhy can‚Äôt LLM spell words? Tokenization. In the video andrej uses finds a token that is one of the longest in GPT-4 tokenizer, it is .DefaultCellStyle, since this is a single token, when asked about its spelling to GPT-4 it fails because it has never seen it indivdually.\nWhy is LLM worse at non-English languages (e.g. Japanese)? Tokenization. The tokenizer is not sufficiently trained on Other languages, so it is tokenized into more tokens. This could also be because of the scracity of the language in the training data of the model itself, but a part of it can actually be attributed to the tokenization.\nWhy is LLM bad at simple arithmetic? Tokenization. Addition is done at character level, but the numbers are split up based on random merges, this is a good blog post exploring the topic.\nWhy did GPT-2 have more than necessary trouble coding in Python? Tokenization. Part of the issue is because the spaces in the python code are tokenized separately, this is a problem with the tokenization itself. This is later fixed with GPT-4 Why did my LLM abruptly halt when it sees the string ‚Äú\u003c|endoftext|\u003e‚Äù? Tokenization. As \u003c|endoftext|\u003e is a special token, it is tokenized as a single token, this could lead to some issues in the model.\nTaken from the video\nWhat is this weird warning I get about a ‚Äútrailing whitespace‚Äù? Tokenization. This is actually very interesting and I would want to disuss this in more detail, partly because this really helps you undersand, why the tokenization is so important.\nSo, Imagine a prompt like this, given to a pretrained model ( not a chat tuned model )\nHere is a tag line for an ice cream shop:_\nThe ‚Äú_‚Äù is used to denote a space, its actually a space. Now the tokenization of it from the tiktoken library is:\n[8586, 374, 264, 4877, 1584, 369, 459, 10054, 12932, 8221, 25, 220] Lets understand why ending the prompt with a space is a problem.\nIn the training data usually the model must have seen the prompt as this\n_Here is a tag line for an ice cream shop:_Oh_yeah!\nObserve that when you tokenize this, _Oh becomes a single token, eseentially the space becomes a part of the token. So when it sees an individual space by itself, it is Out of distribution for the model and it gives a warning.\nEssentially it never knew how to finish teh sequence [25, 220 ] because 220 stands for a space and it would have never occured, because it becomes a part of the word next to it, and becomes a different token.\nThere is a special handling for unstable tokens in the tiktoken code, (in Rust), none of this was documented.\nWhy the LLM break if I ask it about ‚ÄúSolidGoldMagikarp‚Äù? Tokenization. This comes from the famous blog post.\nThe author clustered the embeddings of the tokens and found a speical cluster. When this tokens are included in a prompt the model goes completely off the rails. This is a very interesting observation.\nThe potential theory is that solidGoldmagicKarp is a Reddit user, maybe the tokeinzer training data has a lot of Reddit Data, as a result this became a single token. But in the training data of the model itself this token was never seen. This meant the embedding corresponding to this token was never trained. So, when the model seens this token, it is dealing with a random embedding that was never trained, and hence the model goes off the rails.\nWhy should I prefer to use YAML over JSON with LLMs? Tokenization. JSON is very dense in tokens and YAML is very efficient in tokens. Since we pay per token, we need to be efficient in the tokenization.\nConclusion Tokens are the atoms of LLMs, this is what an LLM sees and it is subtly differnt from the way we( humans ) see the text. As a result different wired behavior could emerge. Thanks to Andrej for making some excellent content. Hopefully I managed to understand the tokenizaion in a bit more in detial while writing this blog. Until next time, Cheers ü•Ç\n","wordCount":"3594","inLanguage":"en","datePublished":"2024-03-04T11:30:03Z","dateModified":"2024-03-04T11:30:03Z","author":{"@type":"Person","name":"Sathvik Joel"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/tech/04032024_tokenizer/"},"publisher":{"@type":"Organization","name":"JoeLogs","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Joe'Logs (Alt + H)">Joe'Logs</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/search/ title="Searchüîç (Alt + /)" accesskey=/><span>Searchüîç</span></a></li><li><a href=http://localhost:1313/posts/ title=Postsüìö><span>Postsüìö</span></a></li><li><a href=http://localhost:1313/archives/ title=Archives‚è±Ô∏è><span>Archives‚è±Ô∏è</span></a></li><li><a href=http://localhost:1313/tags/ title=Tagsüè∑Ô∏è><span>Tagsüè∑Ô∏è</span></a></li><li><a href=http://localhost:1313/posts/life title=Lifeüå±><span>Lifeüå±</span></a></li><li><a href=http://localhost:1313/posts/tech title=Techüë®‚Äçüíª><span>Techüë®‚Äçüíª</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/>Postsüìö</a>&nbsp;¬ª&nbsp;<a href=http://localhost:1313/posts/tech/>Techüë®‚Äçüíª</a></div><h1 class=post-title>Understanding Andrejs Tokenizer Video</h1><div class=post-description>Decoding details from Andrejs video on Tokenizer</div><div class=post-meta>&lt;span title='2024-03-04 11:30:03 +0000 UTC'>March 4, 2024&lt;/span>&amp;nbsp;¬∑&amp;nbsp;17 min&amp;nbsp;¬∑&amp;nbsp;Sathvik Joel</div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#unicode-and-utf-8 aria-label="UNICODE AND UTF-8">UNICODE AND UTF-8</a><ul><li><a href=#important-points-about-unicode-and-utf-8 aria-label="Important Points about UNICODE and UTF-8">Important Points about UNICODE and UTF-8</a></li><li><a href=#related-python-functions aria-label="Related Python Functions">Related Python Functions</a></li></ul></li><li><a href=#tokenizer aria-label=Tokenizer>Tokenizer</a></li><li><a href=#bpe aria-label=BPE>BPE</a><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#motivation aria-label=Motivation>Motivation</a></li><li><a href=#alogorithm aria-label=Alogorithm>Alogorithm</a></li><li><a href=#subtle-modifications-to-bpe aria-label="Subtle modifications to BPE">Subtle modifications to BPE</a></li><li><a href=#what-constitutes-a-trained-tokenizer aria-label="What constitutes a trained tokenizer">What constitutes a trained tokenizer</a></li><li><a href=#using-tiktoken aria-label="Using tiktoken">Using tiktoken</a></li><li><a href=#special-tokens aria-label="Special Tokens">Special Tokens</a></li><li><a href=#how-to-add-special-tokens aria-label="How to add Special Tokens">How to add Special Tokens</a></li><li><a href=#different-modalities aria-label="Different Modalities">Different Modalities</a></li><li><a href=#minbpe aria-label=minBPE>minBPE</a></li></ul></li><li><a href=#sentencepiece aria-label=SentencePiece>SentencePiece</a><ul><li><a href=#options aria-label=Options>Options</a></li><li><a href=#options-in-detail aria-label="Options in Detail">Options in Detail</a></li></ul></li><li><a href=#notes-on-vocab-size aria-label="Notes on vocab size">Notes on <code>vocab size</code></a></li><li><a href=#tokenization-wieirdness aria-label="Tokenization Wieirdness">Tokenization Wieirdness</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p>This blog is based on <a href="https://youtu.be/zduSFxRajkE?si=57oSzM0E5HW3aa_g">Andrejs video on TokenizerüìΩÔ∏è</a>. In an interview with Lex Fridman, Andrej said, he takes 10 hrs to make 1 hr of content, this is me trying to decode the rest of the 9 hours.</p><p>In this I wont be concentrating on the python implementation of BPE algorithm itself. The idea is to uncover the details and subtle points around the video and look closely in this very dense video lecture by Andrej.</p><h2 id=unicode-and-utf-8>UNICODE AND UTF-8<a hidden class=anchor aria-hidden=true href=#unicode-and-utf-8>#</a></h2><p>The whole BPE algorithm is based on the UNICODE and UTF-8 encoding, in this I present a few details that are relavent to understand the BPE algorithm.</p><p>In the video Andrej refer to the <a href=https://en.wikipedia.org/wiki/Unicode>UNICODE</a> and <a href=https://en.wikipedia.org/wiki/UTF-8>UTF-8</a> wikipedia pages, along with <a href=https://www.reedbeta.com/blog/programmers-intro-to-unicode/>this blog post</a> that is more redable. But I persoanlly found <a href=https://tonsky.me/blog/unicode/>this amazing blog</a> that is more reader friendly and gives an easy understanding.</p><h3 id=important-points-about-unicode-and-utf-8>Important Points about UNICODE and UTF-8<a hidden class=anchor aria-hidden=true href=#important-points-about-unicode-and-utf-8>#</a></h3><p>Here are the brief points that are really relavent, taken from <a href=https://tonsky.me/blog/unicode/>this amazing blog</a></p><ol><li>UNICODE is a table that assigns unique numbers to different characters, they keep updating and adding new characters to the table.</li><li>UTF-8 is a way to encode those numbers into bytes. Simply, the way to store the UNICODE characters in memory.</li><li>UTF-8 is a variable length encoding, meaning it can use 1, 2, 3 or 4 bytes to store a character.</li><li>UTF-8 is backward compatible with ASCII, meaning that any ASCII text is also a valid UTF-8 text.</li></ol><p>Again please refer to the resourced above to understand advanced concepts like Grapheme clusters, I persoanlly found the details really worth knowing.</p><h3 id=related-python-functions>Related Python Functions<a hidden class=anchor aria-hidden=true href=#related-python-functions>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>ord</span><span class=p>(</span><span class=s2>&#34;Ìïú&#34;</span><span class=p>)</span>  <span class=c1># Gives the UNICODE number of the character</span>
</span></span><span class=line><span class=cl><span class=c1># 54620</span>
</span></span><span class=line><span class=cl><span class=nb>chr</span><span class=p>(</span><span class=mi>54620</span><span class=p>)</span> <span class=c1># Gives the character corresponding to the UNICODE number</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;Ìïú&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># UTF-8 encoding</span>
</span></span><span class=line><span class=cl><span class=s2>&#34;ÏïàÎÖïÌïòÏÑ∏Ïöî is hello in korean&#34;</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># b&#39;\xec\x95\x88\xeb\x85\x95\xed\x95\x98\xec\x84\xb8\xec\x9a\x94 is hello in korean&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># This gives the bytes representation of the string, use list on the output to get the list in the form of integers</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>list</span><span class=p>(</span><span class=s2>&#34;ÏïàÎÖïÌïòÏÑ∏Ïöî is hello in korean&#34;</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=c1># [236, 149, 136, ...  101, 97, 110]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># UTF-8 decoding</span>
</span></span><span class=line><span class=cl><span class=sa>b</span><span class=s2>&#34;</span><span class=se>\xec\x95\x88\xeb\x85\x95\xed\x95\x98\xec\x84\xb8\xec\x9a\x94</span><span class=s2> is hello in korean&#34;</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;ÏïàÎÖïÌïòÏÑ∏Ïöî is hello in korean&#39;</span>
</span></span><span class=line><span class=cl><span class=c1># If you have a list of integers then use the below method</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>bytes</span><span class=p>([</span><span class=mi>236</span><span class=p>,</span> <span class=mi>149</span><span class=p>,</span> <span class=mi>136</span><span class=p>,</span> <span class=mi>235</span><span class=p>,</span> <span class=mi>133</span><span class=p>,</span> <span class=mi>149</span><span class=p>,</span> <span class=mi>237</span><span class=p>,</span> <span class=mi>149</span><span class=p>,</span> <span class=mi>156</span><span class=p>,</span> <span class=mi>236</span><span class=p>,</span> <span class=mi>151</span><span class=p>,</span> <span class=mi>144</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>105</span><span class=p>,</span> <span class=mi>115</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>104</span><span class=p>,</span> <span class=mi>101</span><span class=p>,</span> <span class=mi>108</span><span class=p>,</span> <span class=mi>108</span><span class=p>,</span> <span class=mi>111</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>105</span><span class=p>,</span> <span class=mi>110</span><span class=p>,</span> <span class=mi>32</span><span class=p>,</span> <span class=mi>107</span><span class=p>,</span> <span class=mi>111</span><span class=p>,</span> <span class=mi>114</span><span class=p>,</span> <span class=mi>101</span><span class=p>,</span> <span class=mi>97</span><span class=p>,</span> <span class=mi>110</span><span class=p>])</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># &#39;ÏïàÎÖïÌïòÏÑ∏Ïöî is hello in korean&#39;</span>
</span></span></code></pre></div><h2 id=tokenizer>Tokenizer<a hidden class=anchor aria-hidden=true href=#tokenizer>#</a></h2><p>This <a href=https://tiktokenizer.vercel.app/>amazing website</a> lets you visualize the Tokenization, this is showcased by andrej in the video. You can play with differnt tokenizers and see how they tokenize the given text.</p><figure><img loading=lazy src=images/tiktokenizer-e1.jpg alt="Leaky ReLU activation function, picture taken from pytorch documentaion of LeakyReLU."><figcaption><p>Leaky ReLU activation function, picture taken from pytorch documentaion of LeakyReLU.</p></figcaption></figure><div id=tiktoken-observations class=toc style=margin-bottom:20px;background:var(--theme)><details><summary><div class=details>Observationsüí°</div></summary><div class=inner><ul><li>Each space is tokenized separately in the python indentaion, this make GPT-2 worse in coding tasks.</li><li>There is no defined rules for tokenizing numbers, they tokenized very randonly</li><li>The word <code>EGG</code> is tokenized differently based on the location in the sentence, casing oof the word</li><li>Korean takes a lot more tokens than english, this is very important observaion, the reason for this is the less amount of korean in the traning data of BPE.</li></ul></div></details></div><h2 id=bpe>BPE<a hidden class=anchor aria-hidden=true href=#bpe>#</a></h2><h3 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h3><p>BPE is a tokenizer that tokenizes the text into subwords.</p><p><em>What is Tokenization ?</em></p><p>Tokenization is the process of breaking the text into smaller parts called tokens. These tokens can be words, characters, or subwords.</p><p>There are various ways to tokenization a text :</p><ol><li>Rule based tokenization, this uncludes using regex to split the text into tokens.</li><li>Subword tokenization, this includes breaking the text into subwords, this is what BPE does.</li></ol><p>This <a href=https://web.archive.org/web/20201101005930/https://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/>wonderful blog</a> dives deep into different tokenization techniques, and is a must read to understand where exactly BPE falls in the whole space of tokenization.</p><h3 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h3><p>Andrej cites that the first mention of BPE in NLP was <a href=https://arxiv.org/abs/1508.07909>here</a>. They first introduced BPE in the context of NMT (Neural Machine transaltion), as at that time most of the NMT models operated on fixed vocabularies ( not the vocabulary in the context of LLMs, they were using word based vacabularies). But translation is an open vocalbulary problem, meaning that the model should be able to translate any word in any language ( not just the ones in their vocabulary). Their motivation comes from the idea that, humans could translate unkown words by breaking them down into smaller parts, and translating those parts, so a sub word segmentation is ideal. They chose BPE and apply it at a chracter level ( unlike at the byte level, that is used for LLMs ).</p><h3 id=alogorithm>Alogorithm<a hidden class=anchor aria-hidden=true href=#alogorithm>#</a></h3><p>This explanation is based on the <a href=https://en.wikipedia.org/wiki/Byte_pair_encoding>wikipedia page</a></p><p><em>The simple version goes like this</em> :</p><p>Iteratively replace the most frequent pair of consecutive symbols with a new symbol, until the desired number of merge operations is reached.</p><div id=bpe-example class=toc style=margin-bottom:20px;background:var(--theme)><details><summary><div class=details>BPE Example üí°</div></summary><div class=inner><p>Here is an example from the same wikipedia page</p><p>Suppose the data to be encoded is</p><blockquote><p>aaabdaaabac</p></blockquote><p>The byte pair &ldquo;aa&rdquo; occurs most often, so it will be replaced by a byte that is not used in the data, such as &ldquo;Z&rdquo;. Now there is the following data and replacement table:</p><blockquote><p>ZabdZabac</p></blockquote><blockquote><p>Z=aa</p></blockquote><p>Then the process is repeated with byte pair &ldquo;ab&rdquo;, replacing it with &ldquo;Y&rdquo;:</p><blockquote><p>ZYdZYac</p></blockquote><blockquote><p>Y=ab</p></blockquote><blockquote><p>Z=aa</p></blockquote><p>The only literal byte pair left occurs only once, and the encoding might stop here. Alternatively, the process could continue with recursive byte pair encoding, replacing &ldquo;ZY&rdquo; with &ldquo;X&rdquo;:</p><blockquote><p>XdXac</p></blockquote><blockquote><p>X=ZY</p></blockquote><blockquote><p>Y=ab</p></blockquote><blockquote><p>Z=aa</p></blockquote><p>This data cannot be compressed further by byte pair encoding because there are no pairs of bytes that occur more than once.</p></div></details></div><p>This algorithm is general and can be applied at any level, depending on your definition of a symbol. In the context of LLMs we use bytes as the symbols. So we merge bytes.</p><p>Andrej also spends 30-40 mins providing a python implementation of the BPE from scratch, I tried it and it was a fun exerciese.</p><p>Here is an imporant observation when BPE is applied to text, the number of merges is a <em>hyper-parameter</em></p><ul><li>$\uparrow$ the number of merges, $\uparrow$ the vocabulary size, meaning the size of the embedding layer and the softmax layer increases.</li><li>$\downarrow$ the number of merges, $\downarrow$ the vocabulary size, meaning that the same text would now would be tokenized into more number of tokens, but attention is costly and we want to attend to the same amount of information keeping the attention cost low.</li></ul><h3 id=subtle-modifications-to-bpe>Subtle modifications to BPE<a hidden class=anchor aria-hidden=true href=#subtle-modifications-to-bpe>#</a></h3><p><em>So BPE is just endcoding the given string in utf-8 and then applying the BPE algorithm on the bytes, right?</em></p><p>You could do it, and thats exaclty what the <code>Basetokensizer()</code> does in <a href=https://github.com/karpathy/minbpe/tree/master/minbpe>minBPE implementation</a>.</p><p>But <em>GPT-2</em> paper mentions a problem with this, and here is the problem ( <em>Taken from the <a href=https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>paper</a> directly</em> )</p><blockquote><p>&ldquo;We observed BPE including many versions of common words like dog since they occur in many variations such as dog. dog! dog? . This results in a sub-optimal allocation of limited vocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any byte sequence.&rdquo;</p></blockquote><p>The final vocabulary size is a <em>hyper-parameter</em>, and this is a trade-off beecause, T</p><p>The larger it is, the bigger the embedding layer is and also the softmax layer gets diluted when making the prediction at the head of the model. So, we really dont want to waste the vocabulary slots on the variations of esentially the same word.</p><p>Hence, they restrict the BPE algorithm to not merge across character categories.</p><p><em>How do they do this ?</em></p><p>While traning, they first split the string using a regex. Below are the regexes taken from the minBPE.</p><div id=regexs class=toc style=margin-bottom:20px;background:var(--theme)><details><summary><div class=details>Regexs</div></summary><div class=inner><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># the main GPT text split patterns, see</span>
</span></span><span class=line><span class=cl><span class=c1># https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py</span>
</span></span><span class=line><span class=cl><span class=n>GPT2_SPLIT_PATTERN</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>&#34;&#34;&#34;&#39;(?:[sdmt]|ll|ve|re)| ?\p</span><span class=si>{L}</span><span class=s2>+| ?\p</span><span class=si>{N}</span><span class=s2>+| ?[^\s\p</span><span class=si>{L}</span><span class=s2>\p</span><span class=si>{N}</span><span class=s2>]+|\s+(?!\S)|\s+&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=n>GPT4_SPLIT_PATTERN</span> <span class=o>=</span> <span class=sa>r</span><span class=s2>&#34;&#34;&#34;&#39;(?i:[sdmt]|ll|ve|re)|[^\r\n\p</span><span class=si>{L}</span><span class=s2>\p</span><span class=si>{N}</span><span class=s2>]?+\p</span><span class=si>{L}</span><span class=s2>+|\p</span><span class=si>{N}</span><span class=s2>{1,3}| ?[^\s\p</span><span class=si>{L}</span><span class=s2>\p</span><span class=si>{N}</span><span class=s2>]++[\r\n]*|\s*[\r\n]|\s+(?!\S)|\s+&#34;&#34;&#34;</span>
</span></span></code></pre></div></div></details></div><p>So before we train, we first chunk the training text using <code>re.findall</code>, then only count the pair frequencies in these chunks, this avoids the merges across the character categories. ( <em>See the changes to train function in <code>RegexTokenizer</code> in minBPE</em> ). Essentially sinece &ldquo;dog?&rdquo; gets split into <code>["dog", "?"]</code> we never would have merged &ldquo;dog?&rdquo; into a single token. ( <em>It took some time for me to understand this, but I hope I got it right</em> )</p><p>The same is done while encoding as well, we chunk, then encode and then merge the tokens.</p><p>Note : <em>GPT-2</em> never relased their training code for BPE (<em>only inference code is availale</em>), the implementation with the regex wont make it complete because, you can observe that in GPT-2 tokenizer tokenizes all spaces independently, but applying the regex and training wont achieve that, this implies that they are enforcing more rules that are not clear.</p><h3 id=what-constitutes-a-trained-tokenizer>What constitutes a trained tokenizer<a hidden class=anchor aria-hidden=true href=#what-constitutes-a-trained-tokenizer>#</a></h3><p>All that is needed to encode and decode are just two things. The <code>vocab</code> and the <code>merges</code>.</p><p><code>vocab</code> is a dictionary that maps the index to the bytes string and <code>merges</code> is a dictionary that maps the pair of tokens to the new token id.</p><p>Here is the merges and vocab from the <em>GPT-2</em></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=err>!</span><span class=n>wget</span> <span class=n>https</span><span class=p>:</span><span class=o>//</span><span class=n>openaipublic</span><span class=o>.</span><span class=n>blob</span><span class=o>.</span><span class=n>core</span><span class=o>.</span><span class=n>windows</span><span class=o>.</span><span class=n>net</span><span class=o>/</span><span class=n>gpt</span><span class=o>-</span><span class=mi>2</span><span class=o>/</span><span class=n>models</span><span class=o>/</span><span class=mi>1558</span><span class=n>M</span><span class=o>/</span><span class=n>vocab</span><span class=o>.</span><span class=n>bpe</span>
</span></span><span class=line><span class=cl><span class=err>!</span><span class=n>wget</span> <span class=n>https</span><span class=p>:</span><span class=o>//</span><span class=n>openaipublic</span><span class=o>.</span><span class=n>blob</span><span class=o>.</span><span class=n>core</span><span class=o>.</span><span class=n>windows</span><span class=o>.</span><span class=n>net</span><span class=o>/</span><span class=n>gpt</span><span class=o>-</span><span class=mi>2</span><span class=o>/</span><span class=n>models</span><span class=o>/</span><span class=mi>1558</span><span class=n>M</span><span class=o>/</span><span class=n>encoder</span><span class=o>.</span><span class=n>json</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span><span class=o>,</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;encoder.json&#39;</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>encoder</span> <span class=o>=</span> <span class=n>json</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>f</span><span class=p>)</span> <span class=c1># &lt;--- ~equivalent to our &#34;vocab&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;vocab.bpe&#39;</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>bpe_data</span> <span class=o>=</span> <span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>bpe_merges</span> <span class=o>=</span> <span class=p>[</span><span class=nb>tuple</span><span class=p>(</span><span class=n>merge_str</span><span class=o>.</span><span class=n>split</span><span class=p>())</span> <span class=k>for</span> <span class=n>merge_str</span> <span class=ow>in</span> <span class=n>bpe_data</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]]</span>
</span></span><span class=line><span class=cl><span class=c1># ^---- ~equivalent to our &#34;merges&#34;</span>
</span></span></code></pre></div><h3 id=using-tiktoken>Using tiktoken<a hidden class=anchor aria-hidden=true href=#using-tiktoken>#</a></h3><p><code>tiktoken</code> is the offical library relased by OpenAI for tokenization, it doesnt contain the training code itself but we can use it to encode and decode with their tokenizers.</p><div id=tiktoken class=toc style=margin-bottom:20px;background:var(--theme)><details><summary><div class=details>Using Tiktoken</div></summary><div class=inner><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tiktoken</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># GPT-2 (does not merge spaces)</span>
</span></span><span class=line><span class=cl><span class=n>enc</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;gpt2&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;    hello world!!!&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># GPT-4 (merges spaces)</span>
</span></span><span class=line><span class=cl><span class=n>enc</span> <span class=o>=</span> <span class=n>tiktoken</span><span class=o>.</span><span class=n>get_encoding</span><span class=p>(</span><span class=s2>&#34;cl100k_base&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>enc</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;    hello world!!!&#34;</span><span class=p>))</span>
</span></span></code></pre></div></div></details></div><h3 id=special-tokens>Special Tokens<a hidden class=anchor aria-hidden=true href=#special-tokens>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>len</span><span class=p>(</span><span class=n>encode</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># 50257</span>
</span></span></code></pre></div><p>If we had 50,000 merges and our base vocabulary was 256 bytes, then we would have 50,256 tokens. But we have 50,257 tokens, this is because of the special token <code>&lt;|endoftext|></code></p><p>When this token is present in the text it is tokenized as a single token and this is not part of BPE algorithm itself and need to be handled speparetly.</p><p><em>GPT-2</em> only has one special token.</p><figure><img loading=lazy src=images/gpt4specialtokens.jpg alt="Speical Tokens for GPT-4 tokenizer taken from tiktoken library code"><figcaption><p>Speical Tokens for GPT-4 tokenizer taken from tiktoken library code</p></figcaption></figure><p><em>GPT-4</em> has 4 special tokens, the <code>FIM</code> tokens are based on this <a href=https://arxiv.org/abs/2207.14255>paper</a></p><p>Note : <em>Tiktoken</em> also supports addition of new tokens, refer to their documentation for more details.</p><h3 id=how-to-add-special-tokens>How to add Special Tokens<a hidden class=anchor aria-hidden=true href=#how-to-add-special-tokens>#</a></h3><p>Some special tokens are added at the start of the training itself, but some are added after the pretrainig ( the research reasons are discussed later down the post ). Adding Special tokens involes a minor model surgery, these are the two steps involved</p><ol><li>Add an extra row in teh embedding layer, and initialize it with random weights.</li><li>Add an extra row in the softmax layer as well.</li></ol><p>This kind of addition is very common, once the model is pre-trained, these special tokens are especially important in later stages for fine-tuning when creating chat based models.</p><figure><img loading=lazy src=images/gpt-3.5-specialtokens.png alt="Speical Tokens for GPT-3.5-turbo. Observe how the &amp;lt;|im_start|&amp;gt;, &amp;lt;|im_end|&amp;gt; tokens are tokenized as a single token"><figcaption><p>Speical Tokens for GPT-3.5-turbo. Observe how the <code>&lt;|im_start|></code>, <code>&lt;|im_end|></code> tokens are tokenized as a single token</p></figcaption></figure><figure><img loading=lazy src=images/llama-special-tokens.jpg alt="Some of the special tokens for llama tokenizer, see that &amp;lt;s&amp;gt;, &amp;lt;/s&amp;gt;, &amp;lt;unk&amp;gt; are special tokens"><figcaption><p>Some of the special tokens for <a href=https://github.com/huggingface/transformers/blob/d45f47ab7f7c31991bb98a0302ded59ab6adac31/src/transformers/models/llama/tokenization_llama.py#L74>llama tokenizer</a>, see that <code>&lt;s></code>, <code>&lt;/s></code>, <code>&lt;unk></code> are special tokens</p></figcaption></figure><p>( <em>TODO : Find an example in code for adding special tokens</em> )</p><p>There is an entire design space around adding new tokens, here is a paper that uses this idea <a href="https://openreview.net/pdf?id=2DtxPCL3T5">Learning to Compress Prompts with Gist Tokens</a> as was mentioned by Andrej in the video.</p><h3 id=different-modalities>Different Modalities<a hidden class=anchor aria-hidden=true href=#different-modalities>#</a></h3><p>So the idea many people are converging to these days is not to change the architecture of the model, but to find a way to tokenize the modality into tokens and feed it to the model.</p><p>For example, in SORA by OpenAI they found a way to tokenize the videos into patches that could be feed to a Model.</p><figure><img loading=lazy src=images/patches.jpg alt="Taken from SORA Technical Paper"><figcaption><p>Taken from SORA Technical Paper</p></figcaption></figure><h3 id=minbpe>minBPE<a hidden class=anchor aria-hidden=true href=#minbpe>#</a></h3><p>The goal of minBPE library is to have tiktoken but with training code.</p><h2 id=sentencepiece>SentencePiece<a hidden class=anchor aria-hidden=true href=#sentencepiece>#</a></h2><p><a href=https://github.com/google/sentencepiece>SentencePiece</a> is a library released by Google, it can is a text tokenizer and supports both BPE and Unigram Language Model( <em>A different alogirhtm for tokenization</em> ). Both LLaMA and Mistral uses SentencePiece for tokenization.</p><p>So the main differnce according to Andrej, between SentencePiece and tiktoken is that, the BPE in sentencePiece is performed directly on &ldquo;UniCode CodePoints&rdquo; and not at the level of Bytes.</p><p>So, <em>the merges happen between codepoints</em>.</p><h3 id=options>Options<a hidden class=anchor aria-hidden=true href=#options>#</a></h3><p><code>character_coverage</code> ( say 0.9995 ) parameter makes sure that if a unicode is rare then it is not included in the vocabulary.</p><p><code>byte_fallback</code> parameter deals with the code points that are excluded from the vocabulary because of the <code>character_coverage</code> parameter. If a code point is not in the vocabulary, then it is encoded ( <em>while infering</em> ), as a sequence of bytes ( <em>according to UTF-8</em> ), or marked as <code>&lt;UNK></code> depending on the value of this parameter.</p><p>Example Demonstrating the usage of Options ( Taken from the Video )</p><h3 id=options-in-detail>Options in Detail<a hidden class=anchor aria-hidden=true href=#options-in-detail>#</a></h3><p>In the below excursion, we train and observe the practical implications of the options in the training of SentencePiece.</p><div id=sentencepiece-observations class=toc style=margin-bottom:20px;background:var(--theme)><details><summary><div class=details>Trainingüí°</div></summary><div class=inner><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sentencepiece</span> <span class=k>as</span> <span class=nn>spm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># write a toy.txt file with some random text</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&#34;toy.txt&#34;</span><span class=p>,</span> <span class=s2>&#34;w&#34;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s2>&#34;utf-8&#34;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s2>&#34;SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Observe that the training data <strong>doesn&rsquo;t include</strong> any korean code points ( for example later )</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># train a sentencepiece model on it</span>
</span></span><span class=line><span class=cl><span class=c1># the settings here are (best effort) those used for training Llama 2</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>options</span> <span class=o>=</span> <span class=nb>dict</span><span class=p>(</span>
</span></span><span class=line><span class=cl>  <span class=c1># input spec</span>
</span></span><span class=line><span class=cl>  <span class=nb>input</span><span class=o>=</span><span class=s2>&#34;toy.txt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>input_format</span><span class=o>=</span><span class=s2>&#34;text&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=c1># output spec</span>
</span></span><span class=line><span class=cl>  <span class=n>model_prefix</span><span class=o>=</span><span class=s2>&#34;tok400&#34;</span><span class=p>,</span> <span class=c1># output filename prefix</span>
</span></span><span class=line><span class=cl>  <span class=c1># algorithm spec</span>
</span></span><span class=line><span class=cl>  <span class=c1># BPE alg</span>
</span></span><span class=line><span class=cl>  <span class=n>model_type</span><span class=o>=</span><span class=s2>&#34;bpe&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>vocab_size</span><span class=o>=</span><span class=mi>400</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=c1># normalization</span>
</span></span><span class=line><span class=cl>  <span class=n>normalization_rule_name</span><span class=o>=</span><span class=s2>&#34;identity&#34;</span><span class=p>,</span> <span class=c1># ew, turn off normalization</span>
</span></span><span class=line><span class=cl>  <span class=n>remove_extra_whitespaces</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>input_sentence_size</span><span class=o>=</span><span class=mi>200000000</span><span class=p>,</span> <span class=c1># max number of training sentences</span>
</span></span><span class=line><span class=cl>  <span class=n>max_sentence_length</span><span class=o>=</span><span class=mi>4192</span><span class=p>,</span> <span class=c1># max number of bytes per sentence</span>
</span></span><span class=line><span class=cl>  <span class=n>seed_sentencepiece_size</span><span class=o>=</span><span class=mi>1000000</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>shuffle_input_sentence</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=c1># rare word treatment</span>
</span></span><span class=line><span class=cl>  <span class=n>character_coverage</span><span class=o>=</span><span class=mf>0.99995</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>byte_fallback</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=c1># merge rules</span>
</span></span><span class=line><span class=cl>  <span class=n>split_digits</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>split_by_unicode_script</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>split_by_whitespace</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>split_by_number</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>max_sentencepiece_length</span><span class=o>=</span><span class=mi>16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>add_dummy_prefix</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>allow_whitespace_only_pieces</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=c1># special tokens</span>
</span></span><span class=line><span class=cl>  <span class=n>unk_id</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=c1># the UNK token MUST exist</span>
</span></span><span class=line><span class=cl>  <span class=n>bos_id</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=c1># the others are optional, set to -1 to turn off</span>
</span></span><span class=line><span class=cl>  <span class=n>eos_id</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=n>pad_id</span><span class=o>=-</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=c1># systems</span>
</span></span><span class=line><span class=cl>  <span class=n>num_threads</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>cpu_count</span><span class=p>(),</span> <span class=c1># use ~all system resources</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>spm</span><span class=o>.</span><span class=n>SentencePieceTrainer</span><span class=o>.</span><span class=n>train</span><span class=p>(</span><span class=o>**</span><span class=n>options</span><span class=p>)</span>
</span></span></code></pre></div></div></details></div><p>Configure the Trainer, <code>byte_fallback</code> is set to True</p><p>In the above drop down we have seen the code for training a SentencePiece model, now lets see how the vocab is organized and how the encoding works.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sp</span> <span class=o>=</span> <span class=n>spm</span><span class=o>.</span><span class=n>SentencePieceProcessor</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>sp</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=s1>&#39;tok400.model&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>vocab</span> <span class=o>=</span> <span class=p>[[</span><span class=n>sp</span><span class=o>.</span><span class=n>id_to_piece</span><span class=p>(</span><span class=n>idx</span><span class=p>),</span> <span class=n>idx</span><span class=p>]</span> <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>sp</span><span class=o>.</span><span class=n>get_piece_size</span><span class=p>())]</span>
</span></span><span class=line><span class=cl><span class=n>vocab</span>
</span></span></code></pre></div><p>We have loaded the model as seen in the above code, we will now see how the vocalbulary is organized.</p><details style=margin-top:10px;margin-bottom:10px><summary>Group 1 : Special Tokens</summary><pre><code>['&lt;unk&gt;', 0],
['&lt;s&gt;', 1],
['&lt;/s&gt;', 2],
</code></pre></details><details style=margin-top:10px;margin-bottom:10px><summary>Group 2 : All the individual bytes because the <code>byte_fallback</code> is set to be true</summary><pre><code>['&lt;0x00&gt;', 3],
['&lt;0x01&gt;', 4],
['&lt;0x02&gt;', 5],
['&lt;0x03&gt;', 6],
['&lt;0x04&gt;', 7],

...

['&lt;0xFB&gt;', 254],
['&lt;0xFC&gt;', 255],
['&lt;0xFD&gt;', 256],
['&lt;0xFE&gt;', 257],
['&lt;0xFF&gt;', 258]
</code></pre></details><details style=margin-top:10px;margin-bottom:10px><summary>Group 3 : The parent Tokens/Merged Tokens</summary><pre><code>['en', 259],
['‚ñÅt', 260],
['ce', 261],
['in', 262],
['ra', 263],

['‚ñÅm', 271],
['‚ñÅu', 272],

['entence', 276],

['‚ñÅthe', 294],
['Piece', 295],
['‚ñÅSentence', 296],
['‚ñÅSentencePiece', 297],
['.]', 298],
['Ne', 299],

['.])', 314],
['age', 315],
['del', 316],

['‚ñÅNe', 323],

['guage', 335],

['‚ñÅtraining', 343],
['.,', 344],
['BP', 345],
['Ku', 346],
['ab', 347],

['lo', 358],
['nr', 359],
['oc', 360]
</code></pre></details><details style=margin-top:10px;margin-bottom:10px><summary>Group 4 : The Individual Code Points that are a part of the training text, the rare ones in the training text are exempted</summary><pre><code>['e', 361],
['‚ñÅ', 362],
[',', 395],
['/', 396],
['B', 397],
['E', 398],
['K', 399]
</code></pre></details><p>Now, lets encode ( <em>infer</em> )</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>ids</span> <span class=o>=</span> <span class=n>sp</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;hello ÏïàÎÖïÌïòÏÑ∏Ïöî&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>ids</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>[362, 378, 361, 372, 358, 362, 239, 152, 139, 238, 136, 152, 240, 152, 155, 239, 135, 187, 239, 157, 151]
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>([</span><span class=n>sp</span><span class=o>.</span><span class=n>id_to_piece</span><span class=p>(</span><span class=n>idx</span><span class=p>)</span> <span class=k>for</span> <span class=n>idx</span> <span class=ow>in</span> <span class=n>ids</span><span class=p>])</span>
</span></span></code></pre></div><pre><code>['‚ñÅ', 'h', 'e', 'l', 'lo', '‚ñÅ', '&lt;0xEC&gt;', '&lt;0x95&gt;', '&lt;0x88&gt;', '&lt;0xEB&gt;', '&lt;0x85&gt;', '&lt;0x95&gt;', '&lt;0xED&gt;', '&lt;0x95&gt;', '&lt;0x98&gt;', '&lt;0xEC&gt;', '&lt;0x84&gt;', '&lt;0xB8&gt;', '&lt;0xEC&gt;', '&lt;0x9A&gt;', '&lt;0x94&gt;']
</code></pre><p>See that the korean characters gets encoded as a sequence of bytes because, those code points done have any mapping in the vocabulary.</p><p>Had the <code>byte_fallback</code> been set to False.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>ids</span> <span class=o>=</span> <span class=n>sp</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s2>&#34;hello ÏïàÎÖïÌïòÏÑ∏Ïöî&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>ids</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>[362, 378, 361, 372, 358, 362, 0]
</code></pre><p>Note that id <code>0</code> stands for <code>UNK</code></p><h2 id=notes-on-vocab-size>Notes on <code>vocab size</code><a hidden class=anchor aria-hidden=true href=#notes-on-vocab-size>#</a></h2><p>In BPE the <code>vocab size</code> is a hyper-parameter. Here are a few questions and answers to them as mentioned by Andrej in his video</p><p><strong>Q. Why cant the <code>vocab size</code> be infinite ?</strong></p><p>In the model definition there are two places where the vocab size appers as show in the figure below:</p><figure><img loading=lazy src=images/vocabsize.jpg alt="vocab size in model definition in gpt.py from andrejs minGPT repo"><figcaption><p><code>vocab size</code> in model definition in gpt.py from andrejs minGPT repo</p></figcaption></figure><p>So as the vocab size increases</p><ol><li>The embedding layer and the lm_head is going to grow in size, this means there are a lot of parameters</li><li>This could mean that these parameters might be under trained, as the model is going to see these tokens more rarely. [ Imagine a rare word such as <code>pneumonoultramicroscopicsilicovolcanoconiosis</code> in the whole training data, the model is going to see this point only once, so the corresponding embedding is going to be under trained ]</li><li>We are squising the information a lot, this is beneficial because we could attend to more information for the same amount of computation, but this also means that the model has to understand a lot of information in the forward pass, which could be a bottleneck.</li></ol><h2 id=tokenization-wieirdness>Tokenization Wieirdness<a hidden class=anchor aria-hidden=true href=#tokenization-wieirdness>#</a></h2><p>Tokenization is at the heart of much weirdness of LLMs</p><hr><ul><li><strong>Why can&rsquo;t LLM spell words? <strong>Tokenization</strong></strong>.</li></ul><p>In the video andrej uses finds a token that is one of the longest in GPT-4 tokenizer, it is <code>.DefaultCellStyle</code>, since this is a single token, when asked about its spelling to GPT-4 it fails because it has never seen it indivdually.</p><hr><ul><li><strong>Why is LLM worse at non-English languages (e.g. Japanese)? <strong>Tokenization</strong></strong>.</li></ul><p>The tokenizer is not sufficiently trained on Other languages, so it is tokenized into more tokens. This could also be because of the scracity of the language in the training data of the model itself, but a part of it can actually be attributed to the tokenization.</p><hr><ul><li><strong>Why is LLM bad at simple arithmetic? <strong>Tokenization</strong></strong>.</li></ul><p>Addition is done at character level, but the numbers are split up based on random merges, this is a <a href=https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/>good blog post</a> exploring the topic.</p><hr><ul><li><strong>Why did GPT-2 have more than necessary trouble coding in Python? <strong>Tokenization</strong></strong>.
Part of the issue is because the spaces in the python code are tokenized separately, this is a problem with the tokenization itself. This is later fixed with GPT-4</li></ul><hr><ul><li><strong>Why did my LLM abruptly halt when it sees the string &ldquo;&lt;|endoftext|>&rdquo;? <strong>Tokenization</strong></strong>.</li></ul><p>As <code>&lt;|endoftext|></code> is a special token, it is tokenized as a single token, this could lead to some issues in the model.</p><figure><img loading=lazy src=images/specialtokens.jpg alt="Taken from the video"><figcaption><p>Taken from the video</p></figcaption></figure><hr><ul><li><strong>What is this weird warning I get about a &ldquo;trailing whitespace&rdquo;? <strong>Tokenization</strong></strong>.</li></ul><p>This is actually very interesting and I would want to disuss this in more detail, partly because this really helps you undersand, why the tokenization is so important.</p><p>So, Imagine a prompt like this, given to a pretrained model ( <em>not a chat tuned model</em> )</p><p><em><code>Here is a tag line for an ice cream shop:_</code></em></p><p>The &ldquo;_&rdquo; is used to denote a space, its actually a space.
Now the tokenization of it from the tiktoken library is:</p><pre><code>[8586, 374, 264, 4877, 1584, 369, 459, 10054, 12932, 8221, 25, 220]
</code></pre><p>Lets understand why ending the prompt with a space is a problem.</p><p>In the training data usually the model must have seen the prompt as this</p><p>_<code>Here is a tag line for an ice cream shop:_Oh_yeah!</code></p><p>Observe that when you tokenize this, <code>_Oh</code> becomes a single token, eseentially the space becomes a part of the token. So when it sees an individual space by itself, it is Out of distribution for the model and it gives a warning.</p><p>Essentially it never knew how to finish teh sequence [25, 220 ] because <code>220</code> stands for a space and it would have never occured, because it becomes a part of the word next to it, and becomes a different token.</p><p>There is a special handling for <code>unstable</code> tokens in the tiktoken code, (<em>in Rust</em>), none of this was documented.</p><hr><ul><li><strong>Why the LLM break if I ask it about &ldquo;SolidGoldMagikarp&rdquo;? <strong>Tokenization</strong></strong>.</li></ul><p>This comes from the <a href=https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation>famous blog post</a>.</p><p>The author clustered the embeddings of the tokens and found a speical cluster. When this tokens are included in a prompt the model goes completely off the rails. This is a very interesting observation.</p><p>The potential theory is that <code>solidGoldmagicKarp</code> is a Reddit user, maybe the tokeinzer training data has a lot of Reddit Data, as a result this became a single token. But in the training data of the model itself this token was never seen. This meant the embedding corresponding to this token was never trained. So, when the model seens this token, it is dealing with a random embedding that was never trained, and hence the model goes off the rails.</p><hr><ul><li><strong>Why should I prefer to use YAML over JSON with LLMs? <strong>Tokenization</strong></strong>.</li></ul><p>JSON is very dense in tokens and YAML is very efficient in tokens. Since we pay per token, we need to be efficient in the tokenization.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Tokens are the atoms of LLMs, this is what an LLM sees and it is subtly differnt from the way we( <em>humans</em> ) see the text. As a result different wired behavior could emerge. <strong>Thanks to Andrej for making some excellent content</strong>. Hopefully I managed to understand the tokenizaion in a bit more in detial while writing this blog. Until next time, Cheers ü•Ç</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/llm/>Llm</a></li><li><a href=http://localhost:1313/tags/math/>Math</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/tech/05032024_activationfunctions/><span class=title>¬´ Prev</span><br><span>Transformer Activation Functions and their Details</span>
</a><a class=next href=http://localhost:1313/posts/life/14112023_toefl/><span class=title>Next ¬ª</span><br><span>My TOEFL preperation Notes</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>JoeLogs</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>