<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Mathematical Musings: Unraveling LLM Subtleties | JoeLogs</title><meta name=keywords content="llm,math"><meta name=description content="In this blog I delve into the intricate mathematical subtilities that abound in a few LLM research papers I recently came across. The primary aim is to unravel these mathematical complexities, offering readers a key to unlocking a deeper comprehension of the complete papers. The focus of this blog remains exclusively on the mathematical nuances, tailored to resonate with those who possess a keen mathematical acumen.
Flash Attention Transformers rely on a core operation called Attention Calculation."><meta name=author content="Sathvik Joel"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.8ae6a4a7f18bb18d6a1046f32bd878daac5ec85ac479d07c4a14f6f75624bdfb.css integrity="sha256-iuakp/GLsY1qEEbzK9h42qxeyFrEedB8ShT291Ykvfs=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity="sha256-rLVP0yu8GYJCi4hQMX5F0Ha5UBJzClk2Zn5rwhd3aSo=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://joelogs.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://joelogs.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://joelogs.com/favicon-32x32.png><link rel=apple-touch-icon href=https://joelogs.com/apple-touch-icon.png><link rel=mask-icon href=https://joelogs.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Mathematical Musings:  Unraveling LLM Subtleties"><meta property="og:description" content="In this blog I delve into the intricate mathematical subtilities that abound in a few LLM research papers I recently came across. The primary aim is to unravel these mathematical complexities, offering readers a key to unlocking a deeper comprehension of the complete papers. The focus of this blog remains exclusively on the mathematical nuances, tailored to resonate with those who possess a keen mathematical acumen.
Flash Attention Transformers rely on a core operation called Attention Calculation."><meta property="og:type" content="article"><meta property="og:url" content="https://joelogs.com/posts/31082023_subtlemathindl/"><meta property="og:image" content="https://joelogs.com/pi.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-08-30T11:30:03+00:00"><meta property="article:modified_time" content="2023-08-30T11:30:03+00:00"><meta property="og:site_name" content="JoeLogs"><meta property="og:see_also" content="https://joelogs.com/posts/03092023_mlresources/"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://joelogs.com/pi.jpg"><meta name=twitter:title content="Mathematical Musings:  Unraveling LLM Subtleties"><meta name=twitter:description content="In this blog I delve into the intricate mathematical subtilities that abound in a few LLM research papers I recently came across. The primary aim is to unravel these mathematical complexities, offering readers a key to unlocking a deeper comprehension of the complete papers. The focus of this blog remains exclusively on the mathematical nuances, tailored to resonate with those who possess a keen mathematical acumen.
Flash Attention Transformers rely on a core operation called Attention Calculation."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://joelogs.com/posts/"},{"@type":"ListItem","position":2,"name":"Mathematical Musings:  Unraveling LLM Subtleties","item":"https://joelogs.com/posts/31082023_subtlemathindl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Mathematical Musings:  Unraveling LLM Subtleties","name":"Mathematical Musings:  Unraveling LLM Subtleties","description":"In this blog I delve into the intricate mathematical subtilities that abound in a few LLM research papers I recently came across. The primary aim is to unravel these mathematical complexities, offering readers a key to unlocking a deeper comprehension of the complete papers. The focus of this blog remains exclusively on the mathematical nuances, tailored to resonate with those who possess a keen mathematical acumen.\nFlash Attention Transformers rely on a core operation called Attention Calculation.","keywords":["llm","math"],"articleBody":"In this blog I delve into the intricate mathematical subtilities that abound in a few LLM research papers I recently came across. The primary aim is to unravel these mathematical complexities, offering readers a key to unlocking a deeper comprehension of the complete papers. The focus of this blog remains exclusively on the mathematical nuances, tailored to resonate with those who possess a keen mathematical acumen.\nFlash Attention Transformers rely on a core operation called Attention Calculation. This basic formula, known as\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\nis essential to transformer models. Making this operation fast on GPUs is really important. Flash Attention addresses this by suggesting a new algorithm that pays attention ( pun intended ) to data movement (IO aware, that is, carefully accounting for reads and writes to different levels of fast and slow memory, e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM ). This has several advantages, like making model training faster and allowing transformers to work well with longer sequences.\nHere are a few references to enhance your understanding of Flash Attention\nAn introductory blog post on Flash Attention by Jackson Cakes A Deep Dive into Flash Attention Paper, A blog by Aleksa Gordić Standard Attention Lets first look at the standard way to calculate and then expand it to the central idea\nLet $q$ be the query, and $k_1, k_2, \\dots, k_n$ be the keys. Correspondingly, let $v_1, v_2, \\dots, v_n$ represent the associated values. In order to illustrate the central concept while avoiding undue distraction, we will initially consider these elements as scalars from the real numbers ($\\mathbb{R}$). However, it’s important to note that the following derivations can be readily extended to accommodate vector representations.\nThe primary objective is to compute the output, denoted as $O \\in \\mathbb{R}$.\nFirst, we define a vector $s$ as follows: $$ s = \\left(q \\cdot k_1, q \\cdot k_2, \\dots, q \\cdot k_n \\right) $$\nIn this formulation, each component $s_i$ of the vector $s$ is calculated by taking the dot product between the query $q$ and the respective key $k_i$.\nSubsequently, we compute a vector $p$ where each component $p_i$ is obtained by applying the exponential function to the corresponding component $s_i$: $$ p = \\left(\\exp(s_1), \\exp(s_2), \\dots, \\exp(s_n) \\right) $$\nHere, the exponential function is applied element-wise to the vector $s$.\nNext, we calculate a scalar $l$, which is the summation of all components of vector $s$: $$ l = \\sum_{i=1}^{n} p_i $$\nNow, employing the concept of softmax, we construct a vector $\\text{softmax}$: $$ \\text{softmax} = \\frac{p}{l} = \\left(\\frac{\\exp(s_1)}{l}, \\frac{\\exp(s_2)}{l}, \\dots, \\frac{\\exp(s_n)}{l} \\right) $$\nFinally, the output $O$ is determined by computing the weighted sum of the values $v_i$ using the weights $w_i$ obtained from the softmax operation: $$ O = w_1 \\cdot v_1 + w_2 \\cdot v_2 + \\dots + w_n \\cdot v_n \\ = \\frac{\\exp(s_1)}{\\sum \\exp(s_i)} \\cdot v_1 + \\frac{\\exp(s_2)}{\\sum \\exp(s_i)} \\cdot v_2 + \\dots + \\frac{\\exp(s_n)}{\\sum \\exp(s_i)} \\cdot v_n $$\nIn this formulation, each value $v_i$ is scaled by its respective weight $w_i$, which is a result of applying the softmax function to the original dot product components $s_i$.\nIncremental Attention Now imagine refining the algorithm we discussed earlier. This time, we still want to calculate $O$ through $n$ steps. However, there’s a twist: in each of the $n$ steps, we only get one set of values, $k_i$ and $v_i$. We’re allowed to use a few tracking variables to help us out.\nMotivation:\nThe reason for adding this rule is tied to some technical stuff like tiling and recomputation, especially when dealing with GPU kernel functions. While we won’t dive into these details here, the original paper Dao et al. has more information. The cool thing is that by solving the problem with this constraint, the actual algorithm in the paper becomes much easier to understand. It’s like looking at the problem through a special lens that can make complex things simpler.\nInitialization: $$ O = 0, \\quad l = 0 $$\n1st Iteration:\nLoad query $q$, key $k_1$, and value $v_1$. Calculate $s_1 = q \\cdot k_1$. Update the summation term: $l_{\\text{new}} = l + \\exp(s_1)$. Update the weighted sum: $O_{\\text{new}} = \\frac{l \\cdot O + \\exp(s_1) \\cdot v_1}{l_{\\text{new}}}$. Update variables: $l = l_{\\text{new}}$ and $O = O_{\\text{new}}$. After these steps, we have: $$ O = \\frac{\\exp(s_1) \\cdot v_1}{\\exp(s_1)} $$\n2nd Iteration:\nLoad query $q$, key $k_2$, and value $v_2$. Calculate $s_2 = q \\cdot k_2$. Update the summation term: $l_{\\text{new}} = l + \\exp(s_2)$. Update the weighted sum: $O_{\\text{new}} = \\frac{l \\cdot O + \\exp(s_2) \\cdot v_2}{l_{\\text{new}}}$. Update variables: $l = l_{\\text{new}}$ and $O = O_{\\text{new}}$. After these steps, we have: $$ O = \\frac{\\exp(s_1) \\cdot v_1 + \\exp(s_2) \\cdot v_2}{\\exp(s_1) + \\exp(s_2)} $$\nExtending this algorithm for $n$ iterations reveals that the final output $O$ coincides with the conventional attention mechanism.\nTrick for Numerical Stability of Softmax The proof provided employs a straightforward method for calculating softmax. However, this approach can result in undesirable outcomes such as generating ‘inf’ (infinity) and ’nan’ (not-a-number) values when implemented in code. To address this issue, a workaround is introduced, which is demonstrated below. For a more in-depth exploration of this topic, you can refer to a blog post authored by Jay Mody, accessible at the following link: Jay Mody’s Blog Post on Stable Softmax.\nTo compute the softmax transformation for a vector \\(s = \\left(s_1, s_2, \\dots, s_n \\right)\\), the following method is applied:\nLet $m = \\max(s) = \\max\\left(s_1, s_2, \\dots, s_n \\right)$.\nShift the values of vector \\(s\\) by \\(m\\), resulting in \\(s^\\sim = \\left(s_1 - m, s_2 - m, \\dots, s_n - m \\right)\\).\nPerform the exponentiation operation on each element of \\(s^\\sim\\), yielding \\(p = \\left(\\exp(s_1 - m), \\exp(s_2 - m), \\dots, \\exp(s_n - m) \\right)\\).\nCalculate the sum of the elements in \\(p\\), denoted as \\(l = \\sum p_i\\).\nFinally, compute the softmax values as the ratio of each \\(p_i\\) to \\(l\\), leading to the softmax transformation: \\(\\text{softmax} = \\left(\\frac{p_1}{l}, \\frac{p_2}{l}, \\dots, \\frac{p_n}{l}\\right)\\).\n$$ \\text{softmax} = \\left(\\frac{\\exp(s_1 - m)}{\\sum_{i = 1}^{n}(s_i - m)}, \\frac{\\exp(s_2 - m)}{\\sum_{i = 1}^{n}(s_i - m)}, \\dots, \\frac{\\exp(s_n - m)}{\\sum_{i = 1}^{n}(s_i - m)}\\right) $$\nIncremental Attention with Trick To incorporate the above mentioned trick, we need to keep track of one more variable called $m$ that represents the maximum. Here is the modified algorithm with the trick\nInitialization: $$ O = 0, \\quad l = 0, \\quad m = -\\infty $$\nith Iteration:\nLoad query $q$, key $k_i$, and value $v_i$. Calculate $s_i = q \\cdot k_i$. Update m, $m_{new} = max\\left(m, s_i \\right)$ Update the summation term: $l_{\\text{new}} = \\exp(m - m_{new})l + \\exp(s_i)$. Update the weighted sum: $O_{\\text{new}} = \\frac{\\exp(m - m_{new})l \\cdot O + \\exp(s_i - m_{new}) \\cdot v_i}{l_{\\text{new}}}$. Update variables: $l = l_{\\text{new}}$, $O = O_{\\text{new}}$ and $m = m_{new}$ After these steps, we have: $$ O = \\frac{\\exp(s_1 - m) \\cdot v_1 + \\exp(s_2 - m) \\cdot v_2 + \\dots exp(s_i - m) \\cdot v_i}{\\exp(s_1 - m) + \\exp(s_2 - m) + \\dots \\exp(s_i -m)} $$\nwhere $m = max\\left(s_1, s_2, \\dots , s_i\\right)$\nExtending this algorithm for $n$ iterations reveals that the final output $O$ coincides with the conventional attention mechanism.\nIncrement in Blocks I made a little simplification in the earlier sections by saying that we have access to only one key value pair in a given iteration, in general we have access to a block of those pairs, i.e., in ith iteration we have access to $\\left(k_{i \\cdot B}, k_{i \\cdot B + 1} \\dots k_{i+1 \\cdot B}\\right)$ and $\\left(v_{i \\cdot B}, v_{i \\cdot B + 1} \\dots v_{i+1 \\cdot B}\\right)$. The extention in this case should also be very obvious from the above steps, Give it a try\nFlashclusion Given the foundation we’ve established, comprehending the actual algorithm discussed in the paper should be straightforward. The sole modification lies in the paper’s approach of handling multiple queries concurrently, as opposed to our singular approach. Regardless, I maintain the suggestion to delve into the original paper for an in-depth understanding.\nFor those intrigued by delving deeper into GPU technology, a highly recommended step would involve enrolling in a comprehensive GPU Programming course. Such a course effectively covers various aspects of GPU kernels and useful techniques like Kernel Fusion and tiling. This provides a robust grasp of the intricate world of GPU programming.\nReferences Dao et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness NeurIPS 2022 ","wordCount":"1412","inLanguage":"en","image":"https://joelogs.com/pi.jpg","datePublished":"2023-08-30T11:30:03Z","dateModified":"2023-08-30T11:30:03Z","author":{"@type":"Person","name":"Sathvik Joel"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://joelogs.com/posts/31082023_subtlemathindl/"},"publisher":{"@type":"Organization","name":"JoeLogs","logo":{"@type":"ImageObject","url":"https://joelogs.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://joelogs.com/ accesskey=h title="Joe'Logs (Alt + H)">Joe'Logs</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://joelogs.com/ title=Posts><span>Posts</span></a></li><li><a href=https://joelogs.com/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://joelogs.com/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://joelogs.com/series/ title=Series><span>Series</span></a></li><li><a href=https://joelogs.com/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Mathematical Musings: Unraveling LLM Subtleties</h1><div class=post-meta><span title='2023-08-30 11:30:03 +0000 +0000'>August 30, 2023</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Sathvik Joel</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#flash-attention>Flash Attention</a><ul><li><a href=#standard-attention>Standard Attention</a></li><li><a href=#incremental-attention>Incremental Attention</a></li><li><a href=#trick-for-numerical-stability-of-softmax>Trick for Numerical Stability of Softmax</a></li><li><a href=#incremental-attention-with-trick>Incremental Attention with Trick</a></li><li><a href=#increment-in-blocks>Increment in Blocks</a></li><li><a href=#flashclusion>Flashclusion</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p>In this blog I delve into the intricate mathematical subtilities that abound in a few LLM research papers I recently came across. The primary aim is to unravel these mathematical complexities, offering readers a key to unlocking a deeper comprehension of the complete papers. The focus of this blog remains exclusively on the mathematical nuances, tailored to resonate with those who possess a keen mathematical acumen.</p><h2 id=flash-attention>Flash Attention<a hidden class=anchor aria-hidden=true href=#flash-attention>#</a></h2><p>Transformers rely on a core operation called Attention Calculation. This basic formula, known as</p><p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$</p><p>is essential to transformer models.
Making this operation fast on GPUs is really important. Flash Attention addresses this by suggesting a new algorithm that pays attention ( <em>pun intended</em> ) to data movement (IO aware, that is, carefully accounting for reads and writes to different levels of fast and slow memory, e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM ). This has several advantages, like making model training faster and allowing transformers to work well with longer sequences.</p><p>Here are a few references to enhance your understanding of Flash Attention</p><ol><li><a href=https://jacksoncakes.com/2023/06/29/flashattention-fast-and-memory-efficient-exact-attentionwith-io-awareness/>An introductory blog post on Flash Attention by Jackson Cakes</a></li><li><a href=https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad>A Deep Dive into Flash Attention Paper, A blog by
Aleksa Gordić</a></li></ol><h3 id=standard-attention>Standard Attention<a hidden class=anchor aria-hidden=true href=#standard-attention>#</a></h3><p>Lets first look at the standard way to calculate and then expand it to the central idea</p><p>Let $q$ be the query, and $k_1, k_2, \dots, k_n$ be the keys. Correspondingly, let $v_1, v_2, \dots, v_n$ represent the associated values. In order to illustrate the central concept while avoiding undue distraction, we will initially consider these elements as scalars from the real numbers ($\mathbb{R}$). However, it&rsquo;s important to note that the following derivations can be readily extended to accommodate vector representations.</p><p>The primary objective is to compute the output, denoted as $O \in \mathbb{R}$.</p><ol><li>First, we define a vector $s$ as follows:</li></ol><p>$$
s = \left(q \cdot k_1, q \cdot k_2, \dots, q \cdot k_n \right)
$$</p><p>In this formulation, each component $s_i$ of the vector $s$ is calculated by taking the dot product between the query $q$ and the respective key $k_i$.</p><ol start=2><li>Subsequently, we compute a vector $p$ where each component $p_i$ is obtained by applying the exponential function to the corresponding component $s_i$:</li></ol><p>$$
p = \left(\exp(s_1), \exp(s_2), \dots, \exp(s_n) \right)
$$</p><p>Here, the exponential function is applied element-wise to the vector $s$.</p><ol start=3><li>Next, we calculate a scalar $l$, which is the summation of all components of vector $s$:</li></ol><p>$$
l = \sum_{i=1}^{n} p_i
$$</p><ol start=4><li>Now, employing the concept of softmax, we construct a vector $\text{softmax}$:</li></ol><p>$$
\text{softmax} = \frac{p}{l} = \left(\frac{\exp(s_1)}{l}, \frac{\exp(s_2)}{l}, \dots, \frac{\exp(s_n)}{l} \right)
$$</p><ol start=5><li>Finally, the output $O$ is determined by computing the weighted sum of the values $v_i$ using the weights $w_i$ obtained from the softmax operation:</li></ol><p>$$
O = w_1 \cdot v_1 + w_2 \cdot v_2 + \dots + w_n \cdot v_n \
= \frac{\exp(s_1)}{\sum \exp(s_i)} \cdot v_1 + \frac{\exp(s_2)}{\sum \exp(s_i)} \cdot v_2 + \dots + \frac{\exp(s_n)}{\sum \exp(s_i)} \cdot v_n
$$</p><p>In this formulation, each value $v_i$ is scaled by its respective weight $w_i$, which is a result of applying the softmax function to the original dot product components $s_i$.</p><h3 id=incremental-attention>Incremental Attention<a hidden class=anchor aria-hidden=true href=#incremental-attention>#</a></h3><p>Now imagine refining the algorithm we discussed earlier. This time, we still want to calculate $O$ through $n$ steps. However, <strong>there&rsquo;s a twist:</strong> in each of the $n$ steps, we only get one set of values, $k_i$ and $v_i$. We&rsquo;re allowed to use a few tracking variables to help us out.</p><p>Motivation:</p><p>The reason for adding this rule is tied to some technical stuff like <a href="https://nichijou.co/cuda7-tiling/#:~:text=tiling%20as%20a%20parallel%20algorithm&amp;text=It%20divides%20the%20long%20access,in%20time%20and%20in%20space.">tiling</a> and recomputation, especially when dealing with GPU kernel functions. While we won&rsquo;t dive into these details here, the original paper <a href=https://arxiv.org/abs/2205.14135>Dao et al.</a> has more information. The cool thing is that by solving the problem with this constraint, the actual algorithm in the paper becomes much easier to understand. It&rsquo;s like looking at the problem through a special lens that can make complex things simpler.</p><p><strong>Initialization:</strong>
$$
O = 0, \quad l = 0
$$</p><p><strong>1st Iteration:</strong></p><ol><li>Load query $q$, key $k_1$, and value $v_1$.</li><li>Calculate $s_1 = q \cdot k_1$.</li><li>Update the summation term: $l_{\text{new}} = l + \exp(s_1)$.</li><li>Update the weighted sum: $O_{\text{new}} = \frac{l \cdot O + \exp(s_1) \cdot v_1}{l_{\text{new}}}$.</li><li>Update variables: $l = l_{\text{new}}$ and $O = O_{\text{new}}$.</li></ol><p>After these steps, we have:
$$
O = \frac{\exp(s_1) \cdot v_1}{\exp(s_1)}
$$</p><p><strong>2nd Iteration:</strong></p><ol><li>Load query $q$, key $k_2$, and value $v_2$.</li><li>Calculate $s_2 = q \cdot k_2$.</li><li>Update the summation term: $l_{\text{new}} = l + \exp(s_2)$.</li><li>Update the weighted sum: $O_{\text{new}} = \frac{l \cdot O + \exp(s_2) \cdot v_2}{l_{\text{new}}}$.</li><li>Update variables: $l = l_{\text{new}}$ and $O = O_{\text{new}}$.</li></ol><p>After these steps, we have:
$$
O = \frac{\exp(s_1) \cdot v_1 + \exp(s_2) \cdot v_2}{\exp(s_1) + \exp(s_2)}
$$</p><p>Extending this algorithm for $n$ iterations reveals that the final output $O$ coincides with the conventional attention mechanism.</p><h3 id=trick-for-numerical-stability-of-softmax>Trick for Numerical Stability of Softmax<a hidden class=anchor aria-hidden=true href=#trick-for-numerical-stability-of-softmax>#</a></h3><p>The proof provided employs a straightforward method for calculating softmax. However, this approach can result in undesirable outcomes such as generating &lsquo;inf&rsquo; (infinity) and &rsquo;nan&rsquo; (not-a-number) values when implemented in code. To address this issue, a workaround is introduced, which is demonstrated below. For a more in-depth exploration of this topic, you can refer to a blog post authored by Jay Mody, accessible at the following link: <a href=https://jaykmody.com/blog/stable-softmax/>Jay Mody&rsquo;s Blog Post on Stable Softmax</a>.</p><p>To compute the softmax transformation for a vector \(s = \left(s_1, s_2, \dots, s_n \right)\), the following method is applied:</p><ol><li><p>Let $m = \max(s) = \max\left(s_1, s_2, \dots, s_n \right)$.</p></li><li><p>Shift the values of vector \(s\) by \(m\), resulting in \(s^\sim = \left(s_1 - m, s_2 - m, \dots, s_n - m \right)\).</p></li><li><p>Perform the exponentiation operation on each element of \(s^\sim\), yielding \(p = \left(\exp(s_1 - m), \exp(s_2 - m), \dots, \exp(s_n - m) \right)\).</p></li><li><p>Calculate the sum of the elements in \(p\), denoted as \(l = \sum p_i\).</p></li><li><p>Finally, compute the softmax values as the ratio of each \(p_i\) to \(l\), leading to the softmax transformation: \(\text{softmax} = \left(\frac{p_1}{l}, \frac{p_2}{l}, \dots, \frac{p_n}{l}\right)\).</p></li></ol><p>$$
\text{softmax} = \left(\frac{\exp(s_1 - m)}{\sum_{i = 1}^{n}(s_i - m)}, \frac{\exp(s_2 - m)}{\sum_{i = 1}^{n}(s_i - m)}, \dots, \frac{\exp(s_n - m)}{\sum_{i = 1}^{n}(s_i - m)}\right)
$$</p><h3 id=incremental-attention-with-trick>Incremental Attention with Trick<a hidden class=anchor aria-hidden=true href=#incremental-attention-with-trick>#</a></h3><p>To incorporate the above mentioned trick, we need to keep track of one more variable called $m$ that represents the maximum. Here is the modified algorithm with the trick</p><p><strong>Initialization:</strong>
$$
O = 0, \quad l = 0, \quad m = -\infty
$$</p><p><strong>ith Iteration:</strong></p><ol><li>Load query $q$, key $k_i$, and value $v_i$.</li><li>Calculate $s_i = q \cdot k_i$.</li><li>Update m, $m_{new} = max\left(m, s_i \right)$</li><li>Update the summation term: $l_{\text{new}} = \exp(m - m_{new})l + \exp(s_i)$.</li><li>Update the weighted sum: $O_{\text{new}} = \frac{\exp(m - m_{new})l \cdot O + \exp(s_i - m_{new}) \cdot v_i}{l_{\text{new}}}$.</li><li>Update variables: $l = l_{\text{new}}$, $O = O_{\text{new}}$ and $m = m_{new}$</li></ol><p>After these steps, we have:
$$
O = \frac{\exp(s_1 - m) \cdot v_1 + \exp(s_2 - m) \cdot v_2 + \dots exp(s_i - m) \cdot v_i}{\exp(s_1 - m) + \exp(s_2 - m) + \dots \exp(s_i -m)}
$$</p><p>where $m = max\left(s_1, s_2, \dots , s_i\right)$</p><p>Extending this algorithm for $n$ iterations reveals that the final output $O$ coincides with the conventional attention mechanism.</p><h3 id=increment-in-blocks>Increment in Blocks<a hidden class=anchor aria-hidden=true href=#increment-in-blocks>#</a></h3><p>I made a little simplification in the earlier sections by saying that we have access to only one key value pair in a given iteration, in general we have access to a block of those pairs, i.e., in ith iteration we have access to $\left(k_{i \cdot B}, k_{i \cdot B + 1} \dots k_{i+1 \cdot B}\right)$ and $\left(v_{i \cdot B}, v_{i \cdot B + 1} \dots v_{i+1 \cdot B}\right)$. The extention in this case should also be very obvious from the above steps, <em>Give it a try</em></p><h3 id=flashclusion>Flashclusion<a hidden class=anchor aria-hidden=true href=#flashclusion>#</a></h3><p>Given the foundation we&rsquo;ve established, comprehending the actual algorithm discussed in the paper should be straightforward. The sole modification lies in the paper&rsquo;s approach of handling multiple queries concurrently, as opposed to our singular approach. Regardless, I maintain the suggestion to delve into the original paper for an in-depth understanding.</p><p>For those intrigued by delving deeper into GPU technology, a highly recommended step would involve enrolling in a comprehensive GPU Programming course. Such a course effectively covers various aspects of GPU kernels and useful techniques like Kernel Fusion and tiling. This provides a robust grasp of the intricate world of GPU programming.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ol><li>Dao et al. <a href=https://arxiv.org/abs/2205.14135>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a> NeurIPS 2022</li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://joelogs.com/tags/llm/>llm</a></li><li><a href=https://joelogs.com/tags/math/>math</a></li></ul><nav class=paginav><a class=prev href=https://joelogs.com/posts/03092023_mlresources/><span class=title>« Prev</span><br><span>Resource Library: My Curated Collection of Useful Materials</span></a>
<a class=next href=https://joelogs.com/posts/22082023_mitacs/><span class=title>Next »</span><br><span>My Mitacs Experience : Part 1</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://joelogs.com/>JoeLogs</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>